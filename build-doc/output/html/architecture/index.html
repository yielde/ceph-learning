
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Architecture &#8212; Ceph Documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/nature.css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/js/ceph.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex/" />
    <link rel="search" title="Search" href="../search/" />
    <link rel="next" title="Contributing to Ceph: A Guide for Developers" href="../dev/developer_guide/" />
    <link rel="prev" title="alerts send" href="../api/mon_command_api/" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../http-routingtable/" title="HTTP Routing Table"
             >routing table</a> |</li>
        <li class="right" >
          <a href="../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../dev/developer_guide/" title="Contributing to Ceph: A Guide for Developers"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../api/mon_command_api/" title="alerts send"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../">Ceph Documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Architecture</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div id="docubetter" align="right" style="padding: 15px; font-weight: bold;">
    <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <section id="architecture">
<h1>Architecture<a class="headerlink" href="#architecture" title="Permalink to this heading">¶</a></h1>
<p><a class="reference internal" href="../glossary/#term-Ceph"><span class="xref std std-term">Ceph</span></a> uniquely delivers <strong>object, block, and file storage</strong> in one
unified system. Ceph is highly reliable, easy to manage, and free. The power of
Ceph can transform your company’s IT infrastructure and your ability to manage
vast amounts of data. Ceph delivers extraordinary scalability–thousands of
clients accessing petabytes to exabytes of data. A <a class="reference internal" href="../glossary/#term-Ceph-Node"><span class="xref std std-term">Ceph Node</span></a> leverages
commodity hardware and intelligent daemons, and a <a class="reference internal" href="../glossary/#term-Ceph-Storage-Cluster"><span class="xref std std-term">Ceph Storage Cluster</span></a>
accommodates large numbers of nodes, which communicate with each other to
replicate and redistribute data dynamically.</p>
<img alt="../_images/stack.png" src="../_images/stack.png" />
<section id="the-ceph-storage-cluster">
<span id="arch-ceph-storage-cluster"></span><h2>The Ceph Storage Cluster<a class="headerlink" href="#the-ceph-storage-cluster" title="Permalink to this heading">¶</a></h2>
<p>Ceph provides an infinitely scalable <a class="reference internal" href="../glossary/#term-Ceph-Storage-Cluster"><span class="xref std std-term">Ceph Storage Cluster</span></a> based upon
<abbr title="Reliable Autonomic Distributed Object Store">RADOS</abbr>, which you can read
about in <a class="reference external" href="https://ceph.io/assets/pdfs/weil-rados-pdsw07.pdf">RADOS - A Scalable, Reliable Storage Service for Petabyte-scale
Storage Clusters</a>.</p>
<p>A Ceph Storage Cluster consists of multiple types of daemons:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../glossary/#term-Ceph-Monitor"><span class="xref std std-term">Ceph Monitor</span></a></p></li>
<li><p><a class="reference internal" href="../glossary/#term-Ceph-OSD-Daemon"><span class="xref std std-term">Ceph OSD Daemon</span></a></p></li>
<li><p><a class="reference internal" href="../glossary/#term-Ceph-Manager"><span class="xref std std-term">Ceph Manager</span></a></p></li>
<li><p><a class="reference internal" href="../glossary/#term-Ceph-Metadata-Server"><span class="xref std std-term">Ceph Metadata Server</span></a></p></li>
</ul>
<p class="ditaa">
<img src="../_images/ditaa-d2b26e342975602e1fa43df2b5dd836dffcdd598.png"/>
</p>
<p>A Ceph Monitor maintains a master copy of the cluster map. A cluster of Ceph
monitors ensures high availability should a monitor daemon fail. Storage cluster
clients retrieve a copy of the cluster map from the Ceph Monitor.</p>
<p>A Ceph OSD Daemon checks its own state and the state of other OSDs and reports
back to monitors.</p>
<p>A Ceph Manager acts as an endpoint for monitoring, orchestration, and plug-in
modules.</p>
<p>A Ceph Metadata Server (MDS) manages file metadata when CephFS is used to
provide file services.</p>
<p>Storage cluster clients and each <a class="reference internal" href="../glossary/#term-Ceph-OSD-Daemon"><span class="xref std std-term">Ceph OSD Daemon</span></a> use the CRUSH algorithm
to efficiently compute information about data location, instead of having to
depend on a central lookup table. Ceph’s high-level features include a
native interface to the Ceph Storage Cluster via <code class="docutils literal notranslate"><span class="pre">librados</span></code>, and a number of
service interfaces built on top of <code class="docutils literal notranslate"><span class="pre">librados</span></code>.</p>
<section id="storing-data">
<h3>Storing Data<a class="headerlink" href="#storing-data" title="Permalink to this heading">¶</a></h3>
<p>The Ceph Storage Cluster receives data from <a class="reference internal" href="../glossary/#term-Ceph-Client"><span class="xref std std-term">Ceph Client</span></a>s–whether it
comes through a <a class="reference internal" href="../glossary/#term-Ceph-Block-Device"><span class="xref std std-term">Ceph Block Device</span></a>, <a class="reference internal" href="../glossary/#term-Ceph-Object-Storage"><span class="xref std std-term">Ceph Object Storage</span></a>, the
<a class="reference internal" href="../glossary/#term-Ceph-File-System"><span class="xref std std-term">Ceph File System</span></a> or a custom implementation you create using
<code class="docutils literal notranslate"><span class="pre">librados</span></code>– which is stored as RADOS objects. Each object is stored on an
<a class="reference internal" href="../glossary/#term-Object-Storage-Device"><span class="xref std std-term">Object Storage Device</span></a>. Ceph OSD Daemons handle read, write, and
replication operations on storage drives.  With the older Filestore back end,
each RADOS object was stored as a separate file on a conventional filesystem
(usually XFS).  With the new and default BlueStore back end, objects are
stored in a monolithic database-like fashion.</p>
<p class="ditaa">
<img src="../_images/ditaa-5a530b3e0aa89fe9a98cf60e943996ec43461eb9.png"/>
</p>
<p>Ceph OSD Daemons store data as objects in a flat namespace (e.g., no
hierarchy of directories). An object has an identifier, binary data, and
metadata consisting of a set of name/value pairs. The semantics are completely
up to <a class="reference internal" href="../glossary/#term-Ceph-Client"><span class="xref std std-term">Ceph Client</span></a>s. For example, CephFS uses metadata to store file
attributes such as the file owner, created date, last modified date, and so
forth.</p>
<p class="ditaa">
<img src="../_images/ditaa-b363b88681891164d307a947109a7d196e259dc8.png"/>
</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>An object ID is unique across the entire cluster, not just the local
filesystem.</p>
</div>
</section>
<section id="scalability-and-high-availability">
<span id="index-0"></span><h3>Scalability and High Availability<a class="headerlink" href="#scalability-and-high-availability" title="Permalink to this heading">¶</a></h3>
<p>In traditional architectures, clients talk to a centralized component (e.g., a
gateway, broker, API, facade, etc.), which acts as a single point of entry to a
complex subsystem. This imposes a limit to both performance and scalability,
while introducing a single point of failure (i.e., if the centralized component
goes down, the whole system goes down, too).</p>
<p>Ceph eliminates the centralized gateway to enable clients to interact with
Ceph OSD Daemons directly. Ceph OSD Daemons create object replicas on other
Ceph Nodes to ensure data safety and high availability. Ceph also uses a cluster
of monitors to ensure high availability. To eliminate centralization, Ceph
uses an algorithm called CRUSH.</p>
<section id="crush-introduction">
<span id="index-1"></span><h4>CRUSH Introduction<a class="headerlink" href="#crush-introduction" title="Permalink to this heading">¶</a></h4>
<p>Ceph Clients and Ceph OSD Daemons both use the <abbr title="Controlled Replication Under Scalable Hashing">CRUSH</abbr> algorithm to efficiently compute
information about object location, instead of having to depend on a
central lookup table. CRUSH provides a better data management mechanism compared
to older approaches, and enables massive scale by cleanly distributing the work
to all the clients and OSD daemons in the cluster. CRUSH uses intelligent data
replication to ensure resiliency, which is better suited to hyper-scale storage.
The following sections provide additional details on how CRUSH works. For a
detailed discussion of CRUSH, see <a class="reference external" href="https://ceph.io/assets/pdfs/weil-crush-sc06.pdf">CRUSH - Controlled, Scalable, Decentralized
Placement of Replicated Data</a>.</p>
</section>
<section id="cluster-map">
<span id="architecture-cluster-map"></span><span id="index-2"></span><h4>Cluster Map<a class="headerlink" href="#cluster-map" title="Permalink to this heading">¶</a></h4>
<p>Ceph depends upon Ceph Clients and Ceph OSD Daemons having knowledge of the
cluster topology, which is inclusive of 5 maps collectively referred to as the
“Cluster Map”:</p>
<ol class="arabic simple">
<li><p><strong>The Monitor Map:</strong> Contains the cluster <code class="docutils literal notranslate"><span class="pre">fsid</span></code>, the position, name
address and port of each monitor. It also indicates the current epoch,
when the map was created, and the last time it changed. To view a monitor
map, execute <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">mon</span> <span class="pre">dump</span></code>.</p></li>
<li><p><strong>The OSD Map:</strong> Contains the cluster <code class="docutils literal notranslate"><span class="pre">fsid</span></code>, when the map was created and
last modified, a list of pools, replica sizes, PG numbers, a list of OSDs
and their status (e.g., <code class="docutils literal notranslate"><span class="pre">up</span></code>, <code class="docutils literal notranslate"><span class="pre">in</span></code>). To view an OSD map, execute
<code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">dump</span></code>.</p></li>
<li><p><strong>The PG Map:</strong> Contains the PG version, its time stamp, the last OSD
map epoch, the full ratios, and details on each placement group such as
the PG ID, the <cite>Up Set</cite>, the <cite>Acting Set</cite>, the state of the PG (e.g.,
<code class="docutils literal notranslate"><span class="pre">active</span> <span class="pre">+</span> <span class="pre">clean</span></code>), and data usage statistics for each pool.</p></li>
<li><p><strong>The CRUSH Map:</strong> Contains a list of storage devices, the failure domain
hierarchy (e.g., device, host, rack, row, room, etc.), and rules for
traversing the hierarchy when storing data. To view a CRUSH map, execute
<code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">getcrushmap</span> <span class="pre">-o</span> <span class="pre">{filename}</span></code>; then, decompile it by executing
<code class="docutils literal notranslate"><span class="pre">crushtool</span> <span class="pre">-d</span> <span class="pre">{comp-crushmap-filename}</span> <span class="pre">-o</span> <span class="pre">{decomp-crushmap-filename}</span></code>.
You can view the decompiled map in a text editor or with <code class="docutils literal notranslate"><span class="pre">cat</span></code>.</p></li>
<li><p><strong>The MDS Map:</strong> Contains the current MDS map epoch, when the map was
created, and the last time it changed. It also contains the pool for
storing metadata, a list of metadata servers, and which metadata servers
are <code class="docutils literal notranslate"><span class="pre">up</span></code> and <code class="docutils literal notranslate"><span class="pre">in</span></code>. To view an MDS map, execute <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">fs</span> <span class="pre">dump</span></code>.</p></li>
</ol>
<p>Each map maintains an iterative history of its operating state changes. Ceph
Monitors maintain a master copy of the cluster map including the cluster
members, state, changes, and the overall health of the Ceph Storage Cluster.</p>
</section>
<section id="high-availability-monitors">
<span id="index-3"></span><h4>High Availability Monitors<a class="headerlink" href="#high-availability-monitors" title="Permalink to this heading">¶</a></h4>
<p>Before Ceph Clients can read or write data, they must contact a Ceph Monitor
to obtain the most recent copy of the cluster map. A Ceph Storage Cluster
can operate with a single monitor; however, this introduces a single
point of failure (i.e., if the monitor goes down, Ceph Clients cannot
read or write data).</p>
<p>For added reliability and fault tolerance, Ceph supports a cluster of monitors.
In a cluster of monitors, latency and other faults can cause one or more
monitors to fall behind the current state of the cluster. For this reason, Ceph
must have agreement among various monitor instances regarding the state of the
cluster. Ceph always uses a majority of monitors (e.g., 1, 2:3, 3:5, 4:6, etc.)
and the <a class="reference external" href="https://en.wikipedia.org/wiki/Paxos_(computer_science)">Paxos</a> algorithm to establish a consensus among the monitors about the
current state of the cluster.</p>
<p>For details on configuring monitors, see the <a class="reference external" href="../rados/configuration/mon-config-ref">Monitor Config Reference</a>.</p>
</section>
<section id="high-availability-authentication">
<span id="arch-high-availability-authentication"></span><span id="index-4"></span><h4>High Availability Authentication<a class="headerlink" href="#high-availability-authentication" title="Permalink to this heading">¶</a></h4>
<p>To identify users and protect against man-in-the-middle attacks, Ceph provides
its <code class="docutils literal notranslate"><span class="pre">cephx</span></code> authentication system to authenticate users and daemons.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">cephx</span></code> protocol does not address data encryption in transport
(e.g., SSL/TLS) or encryption at rest.</p>
</div>
<p>Cephx uses shared secret keys for authentication, meaning both the client and
the monitor cluster have a copy of the client’s secret key. The authentication
protocol is such that both parties are able to prove to each other they have a
copy of the key without actually revealing it. This provides mutual
authentication, which means the cluster is sure the user possesses the secret
key, and the user is sure that the cluster has a copy of the secret key.</p>
<p>A key scalability feature of Ceph is to avoid a centralized interface to the
Ceph object store, which means that Ceph clients must be able to interact with
OSDs directly. To protect data, Ceph provides its <code class="docutils literal notranslate"><span class="pre">cephx</span></code> authentication
system, which authenticates users operating Ceph clients. The <code class="docutils literal notranslate"><span class="pre">cephx</span></code> protocol
operates in a manner with behavior similar to <a class="reference external" href="https://en.wikipedia.org/wiki/Kerberos_(protocol)">Kerberos</a>.</p>
<p>A user/actor invokes a Ceph client to contact a monitor. Unlike Kerberos, each
monitor can authenticate users and distribute keys, so there is no single point
of failure or bottleneck when using <code class="docutils literal notranslate"><span class="pre">cephx</span></code>. The monitor returns an
authentication data structure similar to a Kerberos ticket that contains a
session key for use in obtaining Ceph services.  This session key is itself
encrypted with the user’s permanent  secret key, so that only the user can
request services from the Ceph Monitor(s). The client then uses the session key
to request its desired services from the monitor, and the monitor provides the
client with a ticket that will authenticate the client to the OSDs that actually
handle data. Ceph Monitors and OSDs share a secret, so the client can use the
ticket provided by the monitor with any OSD or metadata server in the cluster.
Like Kerberos, <code class="docutils literal notranslate"><span class="pre">cephx</span></code> tickets expire, so an attacker cannot use an expired
ticket or session key obtained surreptitiously. This form of authentication will
prevent attackers with access to the communications medium from either creating
bogus messages under another user’s identity or altering another user’s
legitimate messages, as long as the user’s secret key is not divulged before it
expires.</p>
<p>To use <code class="docutils literal notranslate"><span class="pre">cephx</span></code>, an administrator must set up users first. In the following
diagram, the <code class="docutils literal notranslate"><span class="pre">client.admin</span></code> user invokes  <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">auth</span> <span class="pre">get-or-create-key</span></code> from
the command line to generate a username and secret key. Ceph’s <code class="docutils literal notranslate"><span class="pre">auth</span></code>
subsystem generates the username and key, stores a copy with the monitor(s) and
transmits the user’s secret back to the <code class="docutils literal notranslate"><span class="pre">client.admin</span></code> user. This means that
the client and the monitor share a secret key.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">client.admin</span></code> user must provide the user ID and
secret key to the user in a secure manner.</p>
</div>
<p class="ditaa">
<img src="../_images/ditaa-98e822f6a4f486de7dc55635f9fb80d356ad931f.png"/>
</p>
<p>To authenticate with the monitor, the client passes in the user name to the
monitor, and the monitor generates a session key and encrypts it with the secret
key associated to the user name. Then, the monitor transmits the encrypted
ticket back to the client. The client then decrypts the payload with the shared
secret key to retrieve the session key. The session key identifies the user for
the current session. The client then requests a ticket on behalf of the user
signed by the session key. The monitor generates a ticket, encrypts it with the
user’s secret key and transmits it back to the client. The client decrypts the
ticket and uses it to sign requests to OSDs and metadata servers throughout the
cluster.</p>
<p class="ditaa">
<img src="../_images/ditaa-22b3096a0b880cfcdc7995b8d870653c71bd5244.png"/>
</p>
<p>The <code class="docutils literal notranslate"><span class="pre">cephx</span></code> protocol authenticates ongoing communications between the client
machine and the Ceph servers. Each message sent between a client and server,
subsequent to the initial authentication, is signed using a ticket that the
monitors, OSDs and metadata servers can verify with their shared secret.</p>
<p class="ditaa">
<img src="../_images/ditaa-3a51d20eaaf90e1071e7dc84ea1fd784896d4b99.png"/>
</p>
<p>The protection offered by this authentication is between the Ceph client and the
Ceph server hosts. The authentication is not extended beyond the Ceph client. If
the user accesses the Ceph client from a remote host, Ceph authentication is not
applied to the connection between the user’s host and the client host.</p>
<p>For configuration details, see <a class="reference external" href="../rados/configuration/auth-config-ref">Cephx Config Guide</a>. For user management
details, see <a class="reference external" href="../rados/operations/user-management">User Management</a>.</p>
</section>
<section id="smart-daemons-enable-hyperscale">
<span id="index-5"></span><h4>Smart Daemons Enable Hyperscale<a class="headerlink" href="#smart-daemons-enable-hyperscale" title="Permalink to this heading">¶</a></h4>
<p>In many clustered architectures, the primary purpose of cluster membership is
so that a centralized interface knows which nodes it can access. Then the
centralized interface provides services to the client through a double
dispatch–which is a <strong>huge</strong> bottleneck at the petabyte-to-exabyte scale.</p>
<p>Ceph eliminates the bottleneck: Ceph’s OSD Daemons AND Ceph Clients are cluster
aware. Like Ceph clients, each Ceph OSD Daemon knows about other Ceph OSD
Daemons in the cluster.  This enables Ceph OSD Daemons to interact directly with
other Ceph OSD Daemons and Ceph Monitors. Additionally, it enables Ceph Clients
to interact directly with Ceph OSD Daemons.</p>
<p>The ability of Ceph Clients, Ceph Monitors and Ceph OSD Daemons to interact with
each other means that Ceph OSD Daemons can utilize the CPU and RAM of the Ceph
nodes to easily perform tasks that would bog down a centralized server. The
ability to leverage this computing power leads to several major benefits:</p>
<ol class="arabic">
<li><p><strong>OSDs Service Clients Directly:</strong> Since any network device has a limit to
the number of concurrent connections it can support, a centralized system
has a low physical limit at high scales. By enabling Ceph Clients to contact
Ceph OSD Daemons directly, Ceph increases both performance and total system
capacity simultaneously, while removing a single point of failure. Ceph
Clients can maintain a session when they need to, and with a particular Ceph
OSD Daemon instead of a centralized server.</p></li>
<li><p><strong>OSD Membership and Status</strong>: Ceph OSD Daemons join a cluster and report
on their status. At the lowest level, the Ceph OSD Daemon status is <code class="docutils literal notranslate"><span class="pre">up</span></code>
or <code class="docutils literal notranslate"><span class="pre">down</span></code> reflecting whether or not it is running and able to service
Ceph Client requests. If a Ceph OSD Daemon is <code class="docutils literal notranslate"><span class="pre">down</span></code> and <code class="docutils literal notranslate"><span class="pre">in</span></code> the Ceph
Storage Cluster, this status may indicate the failure of the Ceph OSD
Daemon. If a Ceph OSD Daemon is not running (e.g., it crashes), the Ceph OSD
Daemon cannot notify the Ceph Monitor that it is <code class="docutils literal notranslate"><span class="pre">down</span></code>. The OSDs
periodically send messages to the Ceph Monitor (<code class="docutils literal notranslate"><span class="pre">MPGStats</span></code> pre-luminous,
and a new <code class="docutils literal notranslate"><span class="pre">MOSDBeacon</span></code> in luminous).  If the Ceph Monitor doesn’t see that
message after a configurable period of time then it marks the OSD down.
This mechanism is a failsafe, however. Normally, Ceph OSD Daemons will
determine if a neighboring OSD is down and report it to the Ceph Monitor(s).
This assures that Ceph Monitors are lightweight processes.  See <a class="reference external" href="../rados/operations/monitoring-osd-pg/#monitoring-osds">Monitoring
OSDs</a> and <a class="reference external" href="../rados/configuration/mon-osd-interaction">Heartbeats</a> for additional details.</p></li>
<li><p><strong>Data Scrubbing:</strong> As part of maintaining data consistency and cleanliness,
Ceph OSD Daemons can scrub objects. That is, Ceph OSD Daemons can compare
their local objects metadata with its replicas stored on other OSDs. Scrubbing
happens on a per-Placement Group base. Scrubbing (usually performed daily)
catches mismatches in size and other metadata. Ceph OSD Daemons also perform deeper
scrubbing by comparing data in objects bit-for-bit with their checksums.
Deep scrubbing (usually performed weekly) finds bad sectors on a drive that
weren’t apparent in a light scrub. See <a class="reference external" href="../rados/configuration/osd-config-ref#scrubbing">Data Scrubbing</a> for details on
configuring scrubbing.</p></li>
<li><p><strong>Replication:</strong> Like Ceph Clients, Ceph OSD Daemons use the CRUSH
algorithm, but the Ceph OSD Daemon uses it to compute where replicas of
objects should be stored (and for rebalancing). In a typical write scenario,
a client uses the CRUSH algorithm to compute where to store an object, maps
the object to a pool and placement group, then looks at the CRUSH map to
identify the primary OSD for the placement group.</p>
<p>The client writes the object to the identified placement group in the
primary OSD. Then, the primary OSD with its own copy of the CRUSH map
identifies the secondary and tertiary OSDs for replication purposes, and
replicates the object to the appropriate placement groups in the secondary
and tertiary OSDs (as many OSDs as additional replicas), and responds to the
client once it has confirmed the object was stored successfully.</p>
</li>
</ol>
<p class="ditaa">
<img src="../_images/ditaa-accd039a93bf169d612159bd97189a33faa6d914.png"/>
</p>
<p>With the ability to perform data replication, Ceph OSD Daemons relieve Ceph
clients from that duty, while ensuring high data availability and data safety.</p>
</section>
</section>
<section id="dynamic-cluster-management">
<h3>Dynamic Cluster Management<a class="headerlink" href="#dynamic-cluster-management" title="Permalink to this heading">¶</a></h3>
<p>In the <a class="reference internal" href="#scalability-and-high-availability">Scalability and High Availability</a> section, we explained how Ceph uses
CRUSH, cluster awareness and intelligent daemons to scale and maintain high
availability. Key to Ceph’s design is the autonomous, self-healing, and
intelligent Ceph OSD Daemon. Let’s take a deeper look at how CRUSH works to
enable modern cloud storage infrastructures to place data, rebalance the cluster
and recover from faults dynamically.</p>
<section id="about-pools">
<span id="index-6"></span><h4>About Pools<a class="headerlink" href="#about-pools" title="Permalink to this heading">¶</a></h4>
<p>The Ceph storage system supports the notion of ‘Pools’, which are logical
partitions for storing objects.</p>
<p>Ceph Clients retrieve a <a class="reference internal" href="#cluster-map">Cluster Map</a> from a Ceph Monitor, and write objects to
pools. The pool’s <code class="docutils literal notranslate"><span class="pre">size</span></code> or number of replicas, the CRUSH rule and the
number of placement groups determine how Ceph will place the data.</p>
<p class="ditaa">
<img src="../_images/ditaa-740d576a80d28d8482b9c550c6aa120b958be46d.png"/>
</p>
<p>Pools set at least the following parameters:</p>
<ul class="simple">
<li><p>Ownership/Access to Objects</p></li>
<li><p>The Number of Placement Groups, and</p></li>
<li><p>The CRUSH Rule to Use.</p></li>
</ul>
<p>See <a class="reference external" href="../rados/operations/pools#set-pool-values">Set Pool Values</a> for details.</p>
</section>
<section id="mapping-pgs-to-osds">
<h4>Mapping PGs to OSDs<a class="headerlink" href="#mapping-pgs-to-osds" title="Permalink to this heading">¶</a></h4>
<p>Each pool has a number of placement groups. CRUSH maps PGs to OSDs dynamically.
When a Ceph Client stores objects, CRUSH will map each object to a placement
group.</p>
<p>Mapping objects to placement groups creates a layer of indirection between the
Ceph OSD Daemon and the Ceph Client. The Ceph Storage Cluster must be able to
grow (or shrink) and rebalance where it stores objects dynamically. If the Ceph
Client “knew” which Ceph OSD Daemon had which object, that would create a tight
coupling between the Ceph Client and the Ceph OSD Daemon. Instead, the CRUSH
algorithm maps each object to a placement group and then maps each placement
group to one or more Ceph OSD Daemons. This layer of indirection allows Ceph to
rebalance dynamically when new Ceph OSD Daemons and the underlying OSD devices
come online. The following diagram depicts how CRUSH maps objects to placement
groups, and placement groups to OSDs.</p>
<p class="ditaa">
<img src="../_images/ditaa-45f879e97a08c72aa96aa7c7b94f465611ff941b.png"/>
</p>
<p>With a copy of the cluster map and the CRUSH algorithm, the client can compute
exactly which OSD to use when reading or writing a particular object.</p>
</section>
<section id="calculating-pg-ids">
<span id="index-7"></span><h4>Calculating PG IDs<a class="headerlink" href="#calculating-pg-ids" title="Permalink to this heading">¶</a></h4>
<p>When a Ceph Client binds to a Ceph Monitor, it retrieves the latest copy of the
<a class="reference internal" href="#cluster-map">Cluster Map</a>. With the cluster map, the client knows about all of the monitors,
OSDs, and metadata servers in the cluster. <strong>However, it doesn’t know anything
about object locations.</strong></p>
<blockquote class="epigraph">
<div><p>Object locations get computed.</p>
</div></blockquote>
<p>The only input required by the client is the object ID and the pool.
It’s simple: Ceph stores data in named pools (e.g., “liverpool”). When a client
wants to store a named object (e.g., “john,” “paul,” “george,” “ringo”, etc.)
it calculates a placement group using the object name, a hash code, the
number of PGs in the pool and the pool name. Ceph clients use the following
steps to compute PG IDs.</p>
<ol class="arabic simple">
<li><p>The client inputs the pool name and the object ID. (e.g., pool = “liverpool”
and object-id = “john”)</p></li>
<li><p>Ceph takes the object ID and hashes it.</p></li>
<li><p>Ceph calculates the hash modulo the number of PGs. (e.g., <code class="docutils literal notranslate"><span class="pre">58</span></code>) to get
a PG ID.</p></li>
<li><p>Ceph gets the pool ID given the pool name (e.g., “liverpool” = <code class="docutils literal notranslate"><span class="pre">4</span></code>)</p></li>
<li><p>Ceph prepends the pool ID to the PG ID (e.g., <code class="docutils literal notranslate"><span class="pre">4.58</span></code>).</p></li>
</ol>
<p>Computing object locations is much faster than performing object location query
over a chatty session. The <abbr title="Controlled Replication Under Scalable Hashing">CRUSH</abbr> algorithm allows a client to compute where objects <em>should</em> be stored,
and enables the client to contact the primary OSD to store or retrieve the
objects.</p>
</section>
<section id="peering-and-sets">
<span id="index-8"></span><h4>Peering and Sets<a class="headerlink" href="#peering-and-sets" title="Permalink to this heading">¶</a></h4>
<p>In previous sections, we noted that Ceph OSD Daemons check each other’s
heartbeats and report back to the Ceph Monitor. Another thing Ceph OSD daemons
do is called ‘peering’, which is the process of bringing all of the OSDs that
store a Placement Group (PG) into agreement about the state of all of the
objects (and their metadata) in that PG. In fact, Ceph OSD Daemons <a class="reference external" href="../rados/configuration/mon-osd-interaction#osds-report-peering-failure">Report
Peering Failure</a> to the Ceph Monitors. Peering issues  usually resolve
themselves; however, if the problem persists, you may need to refer to the
<a class="reference external" href="../rados/troubleshooting/troubleshooting-pg#placement-group-down-peering-failure">Troubleshooting Peering Failure</a> section.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Agreeing on the state does not mean that the PGs have the latest contents.</p>
</div>
<p>The Ceph Storage Cluster was designed to store at least two copies of an object
(i.e., <code class="docutils literal notranslate"><span class="pre">size</span> <span class="pre">=</span> <span class="pre">2</span></code>), which is the minimum requirement for data safety. For high
availability, a Ceph Storage Cluster should store more than two copies of an object
(e.g., <code class="docutils literal notranslate"><span class="pre">size</span> <span class="pre">=</span> <span class="pre">3</span></code> and <code class="docutils literal notranslate"><span class="pre">min</span> <span class="pre">size</span> <span class="pre">=</span> <span class="pre">2</span></code>) so that it can continue to run in a
<code class="docutils literal notranslate"><span class="pre">degraded</span></code> state while maintaining data safety.</p>
<p>Referring back to the diagram in <a class="reference internal" href="#smart-daemons-enable-hyperscale">Smart Daemons Enable Hyperscale</a>, we do not
name the Ceph OSD Daemons specifically (e.g., <code class="docutils literal notranslate"><span class="pre">osd.0</span></code>, <code class="docutils literal notranslate"><span class="pre">osd.1</span></code>, etc.), but
rather refer to them as <em>Primary</em>, <em>Secondary</em>, and so forth. By convention,
the <em>Primary</em> is the first OSD in the <em>Acting Set</em>, and is responsible for
coordinating the peering process for each placement group where it acts as
the <em>Primary</em>, and is the <strong>ONLY</strong> OSD that that will accept client-initiated
writes to objects for a given placement group where it acts as the <em>Primary</em>.</p>
<p>When a series of OSDs are responsible for a placement group, that series of
OSDs, we refer to them as an <em>Acting Set</em>. An <em>Acting Set</em> may refer to the Ceph
OSD Daemons that are currently responsible for the placement group, or the Ceph
OSD Daemons that were responsible  for a particular placement group as of some
epoch.</p>
<p>The Ceph OSD daemons that are part of an <em>Acting Set</em> may not always be  <code class="docutils literal notranslate"><span class="pre">up</span></code>.
When an OSD in the <em>Acting Set</em> is <code class="docutils literal notranslate"><span class="pre">up</span></code>, it is part of the  <em>Up Set</em>. The <em>Up
Set</em> is an important distinction, because Ceph can remap PGs to other Ceph OSD
Daemons when an OSD fails.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In an <em>Acting Set</em> for a PG containing <code class="docutils literal notranslate"><span class="pre">osd.25</span></code>, <code class="docutils literal notranslate"><span class="pre">osd.32</span></code> and
<code class="docutils literal notranslate"><span class="pre">osd.61</span></code>, the first OSD, <code class="docutils literal notranslate"><span class="pre">osd.25</span></code>, is the <em>Primary</em>. If that OSD fails,
the Secondary, <code class="docutils literal notranslate"><span class="pre">osd.32</span></code>, becomes the <em>Primary</em>, and <code class="docutils literal notranslate"><span class="pre">osd.25</span></code> will be
removed from the <em>Up Set</em>.</p>
</div>
</section>
<section id="rebalancing">
<span id="index-9"></span><h4>Rebalancing<a class="headerlink" href="#rebalancing" title="Permalink to this heading">¶</a></h4>
<p>When you add a Ceph OSD Daemon to a Ceph Storage Cluster, the cluster map gets
updated with the new OSD. Referring back to <a class="reference internal" href="#calculating-pg-ids">Calculating PG IDs</a>, this changes
the cluster map. Consequently, it changes object placement, because it changes
an input for the calculations. The following diagram depicts the rebalancing
process (albeit rather crudely, since it is substantially less impactful with
large clusters) where some, but not all of the PGs migrate from existing OSDs
(OSD 1, and OSD 2) to the new OSD (OSD 3). Even when rebalancing, CRUSH is
stable. Many of the placement groups remain in their original configuration,
and each OSD gets some added capacity, so there are no load spikes on the
new OSD after rebalancing is complete.</p>
<p class="ditaa">
<img src="../_images/ditaa-cb68a3f796f66334ea3d88390052d4c06d98028b.png"/>
</p>
</section>
<section id="data-consistency">
<span id="index-10"></span><h4>Data Consistency<a class="headerlink" href="#data-consistency" title="Permalink to this heading">¶</a></h4>
<p>As part of maintaining data consistency and cleanliness, Ceph OSDs also scrub
objects within placement groups. That is, Ceph OSDs compare object metadata in
one placement group with its replicas in placement groups stored in other
OSDs. Scrubbing (usually performed daily) catches OSD bugs or filesystem
errors, often as a result of hardware issues.  OSDs also perform deeper
scrubbing by comparing data in objects bit-for-bit.  Deep scrubbing (by default
performed weekly) finds bad blocks on a drive that weren’t apparent in a light
scrub.</p>
<p>See <a class="reference external" href="../rados/configuration/osd-config-ref#scrubbing">Data Scrubbing</a> for details on configuring scrubbing.</p>
</section>
</section>
<section id="erasure-coding">
<span id="index-11"></span><h3>Erasure Coding<a class="headerlink" href="#erasure-coding" title="Permalink to this heading">¶</a></h3>
<p>An erasure coded pool stores each object as <code class="docutils literal notranslate"><span class="pre">K+M</span></code> chunks. It is divided into
<code class="docutils literal notranslate"><span class="pre">K</span></code> data chunks and <code class="docutils literal notranslate"><span class="pre">M</span></code> coding chunks. The pool is configured to have a size
of <code class="docutils literal notranslate"><span class="pre">K+M</span></code> so that each chunk is stored in an OSD in the acting set. The rank of
the chunk is stored as an attribute of the object.</p>
<p>For instance an erasure coded pool can be created to use five OSDs (<code class="docutils literal notranslate"><span class="pre">K+M</span> <span class="pre">=</span> <span class="pre">5</span></code>) and
sustain the loss of two of them (<code class="docutils literal notranslate"><span class="pre">M</span> <span class="pre">=</span> <span class="pre">2</span></code>).</p>
<section id="reading-and-writing-encoded-chunks">
<h4>Reading and Writing Encoded Chunks<a class="headerlink" href="#reading-and-writing-encoded-chunks" title="Permalink to this heading">¶</a></h4>
<p>When the object <strong>NYAN</strong> containing <code class="docutils literal notranslate"><span class="pre">ABCDEFGHI</span></code> is written to the pool, the erasure
encoding function splits the content into three data chunks simply by dividing
the content in three: the first contains <code class="docutils literal notranslate"><span class="pre">ABC</span></code>, the second <code class="docutils literal notranslate"><span class="pre">DEF</span></code> and the
last <code class="docutils literal notranslate"><span class="pre">GHI</span></code>. The content will be padded if the content length is not a multiple
of <code class="docutils literal notranslate"><span class="pre">K</span></code>. The function also creates two coding chunks: the fourth with <code class="docutils literal notranslate"><span class="pre">YXY</span></code>
and the fifth with <code class="docutils literal notranslate"><span class="pre">QGC</span></code>. Each chunk is stored in an OSD in the acting set.
The chunks are stored in objects that have the same name (<strong>NYAN</strong>) but reside
on different OSDs. The order in which the chunks were created must be preserved
and is stored as an attribute of the object (<code class="docutils literal notranslate"><span class="pre">shard_t</span></code>), in addition to its
name. Chunk 1 contains <code class="docutils literal notranslate"><span class="pre">ABC</span></code> and is stored on <strong>OSD5</strong> while chunk 4 contains
<code class="docutils literal notranslate"><span class="pre">YXY</span></code> and is stored on <strong>OSD3</strong>.</p>
<p class="ditaa">
<img src="../_images/ditaa-86952ee961bd79c367ecd85023e44450c2b55b0d.png"/>
</p>
<p>When the object <strong>NYAN</strong> is read from the erasure coded pool, the decoding
function reads three chunks: chunk 1 containing <code class="docutils literal notranslate"><span class="pre">ABC</span></code>, chunk 3 containing
<code class="docutils literal notranslate"><span class="pre">GHI</span></code> and chunk 4 containing <code class="docutils literal notranslate"><span class="pre">YXY</span></code>. Then, it rebuilds the original content
of the object <code class="docutils literal notranslate"><span class="pre">ABCDEFGHI</span></code>. The decoding function is informed that the chunks 2
and 5 are missing (they are called ‘erasures’). The chunk 5 could not be read
because the <strong>OSD4</strong> is out. The decoding function can be called as soon as
three chunks are read: <strong>OSD2</strong> was the slowest and its chunk was not taken into
account.</p>
<p class="ditaa">
<img src="../_images/ditaa-fea104af11f5649826e45dee02f199165d3e5092.png"/>
</p>
</section>
<section id="interrupted-full-writes">
<h4>Interrupted Full Writes<a class="headerlink" href="#interrupted-full-writes" title="Permalink to this heading">¶</a></h4>
<p>In an erasure coded pool, the primary OSD in the up set receives all write
operations. It is responsible for encoding the payload into <code class="docutils literal notranslate"><span class="pre">K+M</span></code> chunks and
sends them to the other OSDs. It is also responsible for maintaining an
authoritative version of the placement group logs.</p>
<p>In the following diagram, an erasure coded placement group has been created with
<code class="docutils literal notranslate"><span class="pre">K</span> <span class="pre">=</span> <span class="pre">2,</span> <span class="pre">M</span> <span class="pre">=</span> <span class="pre">1</span></code> and is supported by three OSDs, two for <code class="docutils literal notranslate"><span class="pre">K</span></code> and one for
<code class="docutils literal notranslate"><span class="pre">M</span></code>. The acting set of the placement group is made of <strong>OSD 1</strong>, <strong>OSD 2</strong> and
<strong>OSD 3</strong>. An object has been encoded and stored in the OSDs : the chunk
<code class="docutils literal notranslate"><span class="pre">D1v1</span></code> (i.e. Data chunk number 1, version 1) is on <strong>OSD 1</strong>, <code class="docutils literal notranslate"><span class="pre">D2v1</span></code> on
<strong>OSD 2</strong> and <code class="docutils literal notranslate"><span class="pre">C1v1</span></code> (i.e. Coding chunk number 1, version 1) on <strong>OSD 3</strong>. The
placement group logs on each OSD are identical (i.e. <code class="docutils literal notranslate"><span class="pre">1,1</span></code> for epoch 1,
version 1).</p>
<p class="ditaa">
<img src="../_images/ditaa-d4d4679be8341ec82717feaa06874b83a1b50772.png"/>
</p>
<p><strong>OSD 1</strong> is the primary and receives a <strong>WRITE FULL</strong> from a client, which
means the payload is to replace the object entirely instead of overwriting a
portion of it. Version 2 (v2) of the object is created to override version 1
(v1). <strong>OSD 1</strong> encodes the payload into three chunks: <code class="docutils literal notranslate"><span class="pre">D1v2</span></code> (i.e. Data
chunk number 1 version 2) will be on <strong>OSD 1</strong>, <code class="docutils literal notranslate"><span class="pre">D2v2</span></code> on <strong>OSD 2</strong> and
<code class="docutils literal notranslate"><span class="pre">C1v2</span></code> (i.e. Coding chunk number 1 version 2) on <strong>OSD 3</strong>. Each chunk is sent
to the target OSD, including the primary OSD which is responsible for storing
chunks in addition to handling write operations and maintaining an authoritative
version of the placement group logs. When an OSD receives the message
instructing it to write the chunk, it also creates a new entry in the placement
group logs to reflect the change. For instance, as soon as <strong>OSD 3</strong> stores
<code class="docutils literal notranslate"><span class="pre">C1v2</span></code>, it adds the entry <code class="docutils literal notranslate"><span class="pre">1,2</span></code> ( i.e. epoch 1, version 2 ) to its logs.
Because the OSDs work asynchronously, some chunks may still be in flight ( such
as <code class="docutils literal notranslate"><span class="pre">D2v2</span></code> ) while others are acknowledged and persisted to storage drives
(such as <code class="docutils literal notranslate"><span class="pre">C1v1</span></code> and <code class="docutils literal notranslate"><span class="pre">D1v1</span></code>).</p>
<p class="ditaa">
<img src="../_images/ditaa-e95fd29465a1576fc8d93ba2b001bb8f5ac057fc.png"/>
</p>
<p>If all goes well, the chunks are acknowledged on each OSD in the acting set and
the logs’ <code class="docutils literal notranslate"><span class="pre">last_complete</span></code> pointer can move from <code class="docutils literal notranslate"><span class="pre">1,1</span></code> to <code class="docutils literal notranslate"><span class="pre">1,2</span></code>.</p>
<p class="ditaa">
<img src="../_images/ditaa-4bec55e3eed07c19b6bae77130bd06eb98d6e060.png"/>
</p>
<p>Finally, the files used to store the chunks of the previous version of the
object can be removed: <code class="docutils literal notranslate"><span class="pre">D1v1</span></code> on <strong>OSD 1</strong>, <code class="docutils literal notranslate"><span class="pre">D2v1</span></code> on <strong>OSD 2</strong> and <code class="docutils literal notranslate"><span class="pre">C1v1</span></code>
on <strong>OSD 3</strong>.</p>
<p class="ditaa">
<img src="../_images/ditaa-fd31eddeb770839cf3ef48a629d2b5bb5eb3ddb5.png"/>
</p>
<p>But accidents happen. If <strong>OSD 1</strong> goes down while <code class="docutils literal notranslate"><span class="pre">D2v2</span></code> is still in flight,
the object’s version 2 is partially written: <strong>OSD 3</strong> has one chunk but that is
not enough to recover. It lost two chunks: <code class="docutils literal notranslate"><span class="pre">D1v2</span></code> and <code class="docutils literal notranslate"><span class="pre">D2v2</span></code> and the
erasure coding parameters <code class="docutils literal notranslate"><span class="pre">K</span> <span class="pre">=</span> <span class="pre">2</span></code>, <code class="docutils literal notranslate"><span class="pre">M</span> <span class="pre">=</span> <span class="pre">1</span></code> require that at least two chunks are
available to rebuild the third. <strong>OSD 4</strong> becomes the new primary and finds that
the <code class="docutils literal notranslate"><span class="pre">last_complete</span></code> log entry (i.e., all objects before this entry were known
to be available on all OSDs in the previous acting set ) is <code class="docutils literal notranslate"><span class="pre">1,1</span></code> and that
will be the head of the new authoritative log.</p>
<p class="ditaa">
<img src="../_images/ditaa-fa0be989e5935a287c921aedead28b09bc2946c4.png"/>
</p>
<p>The log entry 1,2 found on <strong>OSD 3</strong> is divergent from the new authoritative log
provided by <strong>OSD 4</strong>: it is discarded and the file containing the <code class="docutils literal notranslate"><span class="pre">C1v2</span></code>
chunk is removed. The <code class="docutils literal notranslate"><span class="pre">D1v1</span></code> chunk is rebuilt with the <code class="docutils literal notranslate"><span class="pre">decode</span></code> function of
the erasure coding library during scrubbing and stored on the new primary
<strong>OSD 4</strong>.</p>
<p class="ditaa">
<img src="../_images/ditaa-4d35f7a15d582e30a7b2fa61613bd3626b9348cd.png"/>
</p>
<p>See <a class="reference external" href="https://github.com/ceph/ceph/blob/40059e12af88267d0da67d8fd8d9cd81244d8f93/doc/dev/osd_internals/erasure_coding/developer_notes.rst">Erasure Code Notes</a> for additional details.</p>
</section>
</section>
<section id="cache-tiering">
<h3>Cache Tiering<a class="headerlink" href="#cache-tiering" title="Permalink to this heading">¶</a></h3>
<p>A cache tier provides Ceph Clients with better I/O performance for a subset of
the data stored in a backing storage tier. Cache tiering involves creating a
pool of relatively fast/expensive storage devices (e.g., solid state drives)
configured to act as a cache tier, and a backing pool of either erasure-coded
or relatively slower/cheaper devices configured to act as an economical storage
tier. The Ceph objecter handles where to place the objects and the tiering
agent determines when to flush objects from the cache to the backing storage
tier. So the cache tier and the backing storage tier are completely transparent
to Ceph clients.</p>
<p class="ditaa">
<img src="../_images/ditaa-644de96be5ceeacfe47c2ad4fd6748a1bc13f928.png"/>
</p>
<p>See <a class="reference external" href="../rados/operations/cache-tiering">Cache Tiering</a> for additional details.  Note that Cache Tiers can be
tricky and their use is now discouraged.</p>
</section>
<section id="extending-ceph">
<span id="index-12"></span><h3>Extending Ceph<a class="headerlink" href="#extending-ceph" title="Permalink to this heading">¶</a></h3>
<p>You can extend Ceph by creating shared object classes called ‘Ceph Classes’.
Ceph loads <code class="docutils literal notranslate"><span class="pre">.so</span></code> classes stored in the <code class="docutils literal notranslate"><span class="pre">osd</span> <span class="pre">class</span> <span class="pre">dir</span></code> directory dynamically
(i.e., <code class="docutils literal notranslate"><span class="pre">$libdir/rados-classes</span></code> by default). When you implement a class, you
can create new object methods that have the ability to call the native methods
in the Ceph Object Store, or other class methods you incorporate via libraries
or create yourself.</p>
<p>On writes, Ceph Classes can call native or class methods, perform any series of
operations on the inbound data and generate a resulting write transaction  that
Ceph will apply atomically.</p>
<p>On reads, Ceph Classes can call native or class methods, perform any series of
operations on the outbound data and return the data to the client.</p>
<div class="topic">
<p class="topic-title">Ceph Class Example</p>
<p>A Ceph class for a content management system that presents pictures of a
particular size and aspect ratio could take an inbound bitmap image, crop it
to a particular aspect ratio, resize it and embed an invisible copyright or
watermark to help protect the intellectual property; then, save the
resulting bitmap image to the object store.</p>
</div>
<p>See <code class="docutils literal notranslate"><span class="pre">src/objclass/objclass.h</span></code>, <code class="docutils literal notranslate"><span class="pre">src/fooclass.cc</span></code> and <code class="docutils literal notranslate"><span class="pre">src/barclass</span></code> for
exemplary implementations.</p>
</section>
<section id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">¶</a></h3>
<p>Ceph Storage Clusters are dynamic–like a living organism. Whereas, many storage
appliances do not fully utilize the CPU and RAM of a typical commodity server,
Ceph does. From heartbeats, to  peering, to rebalancing the cluster or
recovering from faults,  Ceph offloads work from clients (and from a centralized
gateway which doesn’t exist in the Ceph architecture) and uses the computing
power of the OSDs to perform the work. When referring to <a class="reference external" href="../start/hardware-recommendations">Hardware
Recommendations</a> and the <a class="reference external" href="../rados/configuration/network-config-ref">Network Config Reference</a>,  be cognizant of the
foregoing concepts to understand how Ceph utilizes computing resources.</p>
</section>
</section>
<section id="ceph-protocol">
<span id="index-13"></span><h2>Ceph Protocol<a class="headerlink" href="#ceph-protocol" title="Permalink to this heading">¶</a></h2>
<p>Ceph Clients use the native protocol for interacting with the Ceph Storage
Cluster. Ceph packages this functionality into the <code class="docutils literal notranslate"><span class="pre">librados</span></code> library so that
you can create your own custom Ceph Clients. The following diagram depicts the
basic architecture.</p>
<p class="ditaa">
<img src="../_images/ditaa-2fb9b073781e561c4947b74687285560dde591af.png"/>
</p>
<section id="native-protocol-and-librados">
<h3>Native Protocol and <code class="docutils literal notranslate"><span class="pre">librados</span></code><a class="headerlink" href="#native-protocol-and-librados" title="Permalink to this heading">¶</a></h3>
<p>Modern applications need a simple object storage interface with asynchronous
communication capability. The Ceph Storage Cluster provides a simple object
storage interface with asynchronous communication capability. The interface
provides direct, parallel access to objects throughout the cluster.</p>
<ul class="simple">
<li><p>Pool Operations</p></li>
<li><p>Snapshots and Copy-on-write Cloning</p></li>
<li><p>Read/Write Objects
- Create or Remove
- Entire Object or Byte Range
- Append or Truncate</p></li>
<li><p>Create/Set/Get/Remove XATTRs</p></li>
<li><p>Create/Set/Get/Remove Key/Value Pairs</p></li>
<li><p>Compound operations and dual-ack semantics</p></li>
<li><p>Object Classes</p></li>
</ul>
</section>
<section id="object-watch-notify">
<span id="index-14"></span><h3>Object Watch/Notify<a class="headerlink" href="#object-watch-notify" title="Permalink to this heading">¶</a></h3>
<p>A client can register a persistent interest with an object and keep a session to
the primary OSD open. The client can send a notification message and a payload to
all watchers and receive notification when the watchers receive the
notification. This enables a client to use any object as a
synchronization/communication channel.</p>
<p class="ditaa">
<img src="../_images/ditaa-3ff59492315938be9210b21c160a9df66df0bdbe.png"/>
</p>
</section>
<section id="data-striping">
<span id="index-15"></span><h3>Data Striping<a class="headerlink" href="#data-striping" title="Permalink to this heading">¶</a></h3>
<p>Storage devices have throughput limitations, which impact performance and
scalability. So storage systems often support <a class="reference external" href="https://en.wikipedia.org/wiki/Data_striping">striping</a>–storing sequential
pieces of information across multiple storage devices–to increase throughput
and performance. The most common form of data striping comes from <a class="reference external" href="https://en.wikipedia.org/wiki/RAID">RAID</a>.
The RAID type most similar to Ceph’s striping is <a class="reference external" href="https://en.wikipedia.org/wiki/RAID_0#RAID_0">RAID 0</a>, or a ‘striped
volume’. Ceph’s striping offers the throughput of RAID 0 striping, the
reliability of n-way RAID mirroring and faster recovery.</p>
<p>Ceph provides three types of clients: Ceph Block Device, Ceph File System, and
Ceph Object Storage. A Ceph Client converts its data from the representation
format it provides to its users (a block device image, RESTful objects, CephFS
filesystem directories) into objects for storage in the Ceph Storage Cluster.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The objects Ceph stores in the Ceph Storage Cluster are not striped.
Ceph Object Storage, Ceph Block Device, and the Ceph File System stripe their
data over multiple Ceph Storage Cluster objects. Ceph Clients that write
directly to the Ceph Storage Cluster via <code class="docutils literal notranslate"><span class="pre">librados</span></code> must perform the
striping (and parallel I/O) for themselves to obtain these benefits.</p>
</div>
<p>The simplest Ceph striping format involves a stripe count of 1 object. Ceph
Clients write stripe units to a Ceph Storage Cluster object until the object is
at its maximum capacity, and then create another object for additional stripes
of data. The simplest form of striping may be sufficient for small block device
images, S3 or Swift objects and CephFS files. However, this simple form doesn’t
take maximum advantage of Ceph’s ability to distribute data across placement
groups, and consequently doesn’t improve performance very much. The following
diagram depicts the simplest form of striping:</p>
<p class="ditaa">
<img src="../_images/ditaa-609b2033fcdfa0a95b663189cc63db38953866a1.png"/>
</p>
<p>If you anticipate large images sizes, large S3 or Swift objects (e.g., video),
or large CephFS directories, you may see considerable read/write performance
improvements by striping client data over multiple objects within an object set.
Significant write performance occurs when the client writes the stripe units to
their corresponding objects in parallel. Since objects get mapped to different
placement groups and further mapped to different OSDs, each write occurs in
parallel at the maximum write speed. A write to a single drive would be limited
by the head movement (e.g. 6ms per seek) and bandwidth of that one device (e.g.
100MB/s).  By spreading that write over multiple objects (which map to different
placement groups and OSDs) Ceph can reduce the number of seeks per drive and
combine the throughput of multiple drives to achieve much faster write (or read)
speeds.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Striping is independent of object replicas. Since CRUSH
replicates objects across OSDs, stripes get replicated automatically.</p>
</div>
<p>In the following diagram, client data gets striped across an object set
(<code class="docutils literal notranslate"><span class="pre">object</span> <span class="pre">set</span> <span class="pre">1</span></code> in the following diagram) consisting of 4 objects, where the
first stripe unit is <code class="docutils literal notranslate"><span class="pre">stripe</span> <span class="pre">unit</span> <span class="pre">0</span></code> in <code class="docutils literal notranslate"><span class="pre">object</span> <span class="pre">0</span></code>, and the fourth stripe
unit is <code class="docutils literal notranslate"><span class="pre">stripe</span> <span class="pre">unit</span> <span class="pre">3</span></code> in <code class="docutils literal notranslate"><span class="pre">object</span> <span class="pre">3</span></code>. After writing the fourth stripe, the
client determines if the object set is full. If the object set is not full, the
client begins writing a stripe to the first object again (<code class="docutils literal notranslate"><span class="pre">object</span> <span class="pre">0</span></code> in the
following diagram). If the object set is full, the client creates a new object
set (<code class="docutils literal notranslate"><span class="pre">object</span> <span class="pre">set</span> <span class="pre">2</span></code> in the following diagram), and begins writing to the first
stripe (<code class="docutils literal notranslate"><span class="pre">stripe</span> <span class="pre">unit</span> <span class="pre">16</span></code>) in the first object in the new object set (<code class="docutils literal notranslate"><span class="pre">object</span>
<span class="pre">4</span></code> in the diagram below).</p>
<p class="ditaa">
<img src="../_images/ditaa-96a6fc80dad17fb53f161987ed64f0779930ffe1.png"/>
</p>
<p>Three important variables determine how Ceph stripes data:</p>
<ul class="simple">
<li><p><strong>Object Size:</strong> Objects in the Ceph Storage Cluster have a maximum
configurable size (e.g., 2MB, 4MB, etc.). The object size should be large
enough to accommodate many stripe units, and should be a multiple of
the stripe unit.</p></li>
<li><p><strong>Stripe Width:</strong> Stripes have a configurable unit size (e.g., 64kb).
The Ceph Client divides the data it will write to objects into equally
sized stripe units, except for the last stripe unit. A stripe width,
should be a fraction of the Object Size so that an object may contain
many stripe units.</p></li>
<li><p><strong>Stripe Count:</strong> The Ceph Client writes a sequence of stripe units
over a series of objects determined by the stripe count. The series
of objects is called an object set. After the Ceph Client writes to
the last object in the object set, it returns to the first object in
the object set.</p></li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Test the performance of your striping configuration before
putting your cluster into production. You CANNOT change these striping
parameters after you stripe the data and write it to objects.</p>
</div>
<p>Once the Ceph Client has striped data to stripe units and mapped the stripe
units to objects, Ceph’s CRUSH algorithm maps the objects to placement groups,
and the placement groups to Ceph OSD Daemons before the objects are stored as
files on a storage drive.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since a client writes to a single pool, all data striped into objects
get mapped to placement groups in the same pool. So they use the same CRUSH
map and the same access controls.</p>
</div>
</section>
</section>
<section id="ceph-clients">
<span id="index-16"></span><h2>Ceph Clients<a class="headerlink" href="#ceph-clients" title="Permalink to this heading">¶</a></h2>
<p>Ceph Clients include a number of service interfaces. These include:</p>
<ul class="simple">
<li><p><strong>Block Devices:</strong> The <a class="reference internal" href="../glossary/#term-Ceph-Block-Device"><span class="xref std std-term">Ceph Block Device</span></a> (a.k.a., RBD) service
provides resizable, thin-provisioned block devices with snapshotting and
cloning. Ceph stripes a block device across the cluster for high
performance. Ceph supports both kernel objects (KO) and a QEMU hypervisor
that uses <code class="docutils literal notranslate"><span class="pre">librbd</span></code> directly–avoiding the kernel object overhead for
virtualized systems.</p></li>
<li><p><strong>Object Storage:</strong> The <a class="reference internal" href="../glossary/#term-Ceph-Object-Storage"><span class="xref std std-term">Ceph Object Storage</span></a> (a.k.a., RGW) service
provides RESTful APIs with interfaces that are compatible with Amazon S3
and OpenStack Swift.</p></li>
<li><p><strong>Filesystem</strong>: The <a class="reference internal" href="../glossary/#term-Ceph-File-System"><span class="xref std std-term">Ceph File System</span></a> (CephFS) service provides
a POSIX compliant filesystem usable with <code class="docutils literal notranslate"><span class="pre">mount</span></code> or as
a filesystem in user space (FUSE).</p></li>
</ul>
<p>Ceph can run additional instances of OSDs, MDSs, and monitors for scalability
and high availability. The following diagram depicts the high-level
architecture.</p>
<p class="ditaa">
<img src="../_images/ditaa-0ec10ee2b26b2d5abfa8614819a0934ef41c4cc5.png"/>
</p>
<section id="ceph-object-storage">
<span id="index-17"></span><h3>Ceph Object Storage<a class="headerlink" href="#ceph-object-storage" title="Permalink to this heading">¶</a></h3>
<p>The Ceph Object Storage daemon, <code class="docutils literal notranslate"><span class="pre">radosgw</span></code>, is a FastCGI service that provides
a <a class="reference external" href="https://en.wikipedia.org/wiki/RESTful">RESTful</a> HTTP API to store objects and metadata. It layers on top of the Ceph
Storage Cluster with its own data formats, and maintains its own user database,
authentication, and access control. The RADOS Gateway uses a unified namespace,
which means you can use either the OpenStack Swift-compatible API or the Amazon
S3-compatible API. For example, you can write data using the S3-compatible API
with one application and then read data using the Swift-compatible API with
another application.</p>
<div class="topic">
<p class="topic-title">S3/Swift Objects and Store Cluster Objects Compared</p>
<p>Ceph’s Object Storage uses the term <em>object</em> to describe the data it stores.
S3 and Swift objects are not the same as the objects that Ceph writes to the
Ceph Storage Cluster. Ceph Object Storage objects are mapped to Ceph Storage
Cluster objects. The S3 and Swift objects do not necessarily
correspond in a 1:1 manner with an object stored in the storage cluster. It
is possible for an S3 or Swift object to map to multiple Ceph objects.</p>
</div>
<p>See <a class="reference external" href="../radosgw/">Ceph Object Storage</a> for details.</p>
</section>
<section id="ceph-block-device">
<span id="index-18"></span><h3>Ceph Block Device<a class="headerlink" href="#ceph-block-device" title="Permalink to this heading">¶</a></h3>
<p>A Ceph Block Device stripes a block device image over multiple objects in the
Ceph Storage Cluster, where each object gets mapped to a placement group and
distributed, and the placement groups are spread across separate <code class="docutils literal notranslate"><span class="pre">ceph-osd</span></code>
daemons throughout the cluster.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Striping allows RBD block devices to perform better than a single
server could!</p>
</div>
<p>Thin-provisioned snapshottable Ceph Block Devices are an attractive option for
virtualization and cloud computing. In virtual machine scenarios, people
typically deploy a Ceph Block Device with the <code class="docutils literal notranslate"><span class="pre">rbd</span></code> network storage driver in
QEMU/KVM, where the host machine uses <code class="docutils literal notranslate"><span class="pre">librbd</span></code> to provide a block device
service to the guest. Many cloud computing stacks use <code class="docutils literal notranslate"><span class="pre">libvirt</span></code> to integrate
with hypervisors. You can use thin-provisioned Ceph Block Devices with QEMU and
<code class="docutils literal notranslate"><span class="pre">libvirt</span></code> to support OpenStack and CloudStack among other solutions.</p>
<p>While we do not provide <code class="docutils literal notranslate"><span class="pre">librbd</span></code> support with other hypervisors at this time,
you may also use Ceph Block Device kernel objects to provide a block device to a
client. Other virtualization technologies such as Xen can access the Ceph Block
Device kernel object(s). This is done with the  command-line tool <code class="docutils literal notranslate"><span class="pre">rbd</span></code>.</p>
</section>
<section id="ceph-file-system">
<span id="arch-cephfs"></span><span id="index-19"></span><h3>Ceph File System<a class="headerlink" href="#ceph-file-system" title="Permalink to this heading">¶</a></h3>
<p>The Ceph File System (CephFS) provides a POSIX-compliant filesystem as a
service that is layered on top of the object-based Ceph Storage Cluster.
CephFS files get mapped to objects that Ceph stores in the Ceph Storage
Cluster. Ceph Clients mount a CephFS filesystem as a kernel object or as
a Filesystem in User Space (FUSE).</p>
<p class="ditaa">
<img src="../_images/ditaa-a3cf58afeea95c637ca2c94368599627b433c4ff.png"/>
</p>
<p>The Ceph File System service includes the Ceph Metadata Server (MDS) deployed
with the Ceph Storage cluster. The purpose of the MDS is to store all the
filesystem metadata (directories, file ownership, access modes, etc) in
high-availability Ceph Metadata Servers where the metadata resides in memory.
The reason for the MDS (a daemon called <code class="docutils literal notranslate"><span class="pre">ceph-mds</span></code>) is that simple filesystem
operations like listing a directory or changing a directory (<code class="docutils literal notranslate"><span class="pre">ls</span></code>, <code class="docutils literal notranslate"><span class="pre">cd</span></code>)
would tax the Ceph OSD Daemons unnecessarily. So separating the metadata from
the data means that the Ceph File System can provide high performance services
without taxing the Ceph Storage Cluster.</p>
<p>CephFS separates the metadata from the data, storing the metadata in the MDS,
and storing the file data in one or more objects in the Ceph Storage Cluster.
The Ceph filesystem aims for POSIX compatibility. <code class="docutils literal notranslate"><span class="pre">ceph-mds</span></code> can run as a
single process, or it can be distributed out to multiple physical machines,
either for high availability or for scalability.</p>
<ul class="simple">
<li><p><strong>High Availability</strong>: The extra <code class="docutils literal notranslate"><span class="pre">ceph-mds</span></code> instances can be <cite>standby</cite>,
ready to take over the duties of any failed <code class="docutils literal notranslate"><span class="pre">ceph-mds</span></code> that was
<cite>active</cite>. This is easy because all the data, including the journal, is
stored on RADOS. The transition is triggered automatically by <code class="docutils literal notranslate"><span class="pre">ceph-mon</span></code>.</p></li>
<li><p><strong>Scalability</strong>: Multiple <code class="docutils literal notranslate"><span class="pre">ceph-mds</span></code> instances can be <cite>active</cite>, and they
will split the directory tree into subtrees (and shards of a single
busy directory), effectively balancing the load amongst all <cite>active</cite>
servers.</p></li>
</ul>
<p>Combinations of <cite>standby</cite> and <cite>active</cite> etc are possible, for example
running 3 <cite>active</cite> <code class="docutils literal notranslate"><span class="pre">ceph-mds</span></code> instances for scaling, and one <cite>standby</cite>
instance for high availability.</p>
</section>
</section>
</section>



            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../">
              <img class="logo" src="../_static/logo.png" alt="Logo"/>
            </a></p>
<h3><a href="../">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../start/intro/">Intro to Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/">Installing Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cephadm/">Cephadm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rados/">Ceph Storage Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cephfs/">Ceph File System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rbd/">Ceph Block Device</a></li>
<li class="toctree-l1"><a class="reference internal" href="../radosgw/">Ceph Object Gateway</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mgr/">Ceph Manager Daemon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mgr/dashboard/">Ceph Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/">API Documentation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Architecture</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#the-ceph-storage-cluster">The Ceph Storage Cluster</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#storing-data">Storing Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scalability-and-high-availability">Scalability and High Availability</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#crush-introduction">CRUSH Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cluster-map">Cluster Map</a></li>
<li class="toctree-l4"><a class="reference internal" href="#high-availability-monitors">High Availability Monitors</a></li>
<li class="toctree-l4"><a class="reference internal" href="#high-availability-authentication">High Availability Authentication</a></li>
<li class="toctree-l4"><a class="reference internal" href="#smart-daemons-enable-hyperscale">Smart Daemons Enable Hyperscale</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#dynamic-cluster-management">Dynamic Cluster Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#about-pools">About Pools</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mapping-pgs-to-osds">Mapping PGs to OSDs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#calculating-pg-ids">Calculating PG IDs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#peering-and-sets">Peering and Sets</a></li>
<li class="toctree-l4"><a class="reference internal" href="#rebalancing">Rebalancing</a></li>
<li class="toctree-l4"><a class="reference internal" href="#data-consistency">Data Consistency</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#erasure-coding">Erasure Coding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#reading-and-writing-encoded-chunks">Reading and Writing Encoded Chunks</a></li>
<li class="toctree-l4"><a class="reference internal" href="#interrupted-full-writes">Interrupted Full Writes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#cache-tiering">Cache Tiering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#extending-ceph">Extending Ceph</a></li>
<li class="toctree-l3"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#ceph-protocol">Ceph Protocol</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#native-protocol-and-librados">Native Protocol and <code class="docutils literal notranslate"><span class="pre">librados</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#object-watch-notify">Object Watch/Notify</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-striping">Data Striping</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#ceph-clients">Ceph Clients</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ceph-object-storage">Ceph Object Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ceph-block-device">Ceph Block Device</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ceph-file-system">Ceph File System</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../dev/developer_guide/">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev/internals/">Ceph Internals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../governance/">Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../foundation/">Ceph Foundation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.ceph.com/en/latest/releases/general/">Ceph Releases (general)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.ceph.com/en/latest/releases/">Ceph Releases (index)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../security/">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary/">Glossary</a></li>
</ul>


<!-- ugly kludge to make genindex look like it's part of the toc -->
<ul style="margin-top: -10px"><li class="toctree-l1"><a class="reference internal" href="../genindex/">Index</a></li></ul>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search/" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../http-routingtable/" title="HTTP Routing Table"
             >routing table</a> |</li>
        <li class="right" >
          <a href="../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../dev/developer_guide/" title="Contributing to Ceph: A Guide for Developers"
             >next</a> |</li>
        <li class="right" >
          <a href="../api/mon_command_api/" title="alerts send"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../">Ceph Documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Architecture</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).
    </div>
  </body>
</html>