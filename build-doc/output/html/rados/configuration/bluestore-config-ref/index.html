
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>BlueStore Configuration Reference &#8212; Ceph Documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/nature.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css" />
    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/ceph.js"></script>
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex/" />
    <link rel="search" title="Search" href="../../../search/" />
    <link rel="next" title="Filestore Config Reference" href="../filestore-config-ref/" />
    <link rel="prev" title="mClock Config Reference" href="../mclock-config-ref/" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../http-routingtable/" title="HTTP Routing Table"
             >routing table</a> |</li>
        <li class="right" >
          <a href="../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../filestore-config-ref/" title="Filestore Config Reference"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../mclock-config-ref/" title="mClock Config Reference"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../">Ceph Documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../" >Ceph Storage Cluster</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../" accesskey="U">Configuration</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">BlueStore Configuration Reference</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div id="docubetter" align="right" style="padding: 15px; font-weight: bold;">
    <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <section id="bluestore-configuration-reference">
<h1>BlueStore Configuration Reference<a class="headerlink" href="#bluestore-configuration-reference" title="Permalink to this heading">¶</a></h1>
<section id="devices">
<h2>Devices<a class="headerlink" href="#devices" title="Permalink to this heading">¶</a></h2>
<p>BlueStore manages either one, two, or in certain cases three storage devices.
These <em>devices</em> are “devices” in the Linux/Unix sense. This means that they are
assets listed under <code class="docutils literal notranslate"><span class="pre">/dev</span></code> or <code class="docutils literal notranslate"><span class="pre">/devices</span></code>. Each of these devices may be an
entire storage drive, or a partition of a storage drive, or a logical volume.
BlueStore does not create or mount a conventional file system on devices that
it uses; BlueStore reads and writes to the devices directly in a “raw” fashion.</p>
<p>In the simplest case, BlueStore consumes all of a single storage device. This
device is known as the <em>primary device</em>. The primary device is identified by
the <code class="docutils literal notranslate"><span class="pre">block</span></code> symlink in the data directory.</p>
<p>The data directory is a <code class="docutils literal notranslate"><span class="pre">tmpfs</span></code> mount. When this data directory is booted or
activated by <code class="docutils literal notranslate"><span class="pre">ceph-volume</span></code>, it is populated with metadata files and links
that hold information about the OSD: for example, the OSD’s identifier, the
name of the cluster that the OSD belongs to, and the OSD’s private keyring.</p>
<p>In more complicated cases, BlueStore is deployed across one or two additional
devices:</p>
<ul class="simple">
<li><p>A <em>write-ahead log (WAL) device</em> (identified as <code class="docutils literal notranslate"><span class="pre">block.wal</span></code> in the data
directory) can be used to separate out BlueStore’s internal journal or
write-ahead log. Using a WAL device is advantageous only if the WAL device
is faster than the primary device (for example, if the WAL device is an SSD
and the primary device is an HDD).</p></li>
<li><p>A <em>DB device</em> (identified as <code class="docutils literal notranslate"><span class="pre">block.db</span></code> in the data directory) can be used
to store BlueStore’s internal metadata. BlueStore (or more precisely, the
embedded RocksDB) will put as much metadata as it can on the DB device in
order to improve performance. If the DB device becomes full, metadata will
spill back onto the primary device (where it would have been located in the
absence of the DB device). Again, it is advantageous to provision a DB device
only if it is faster than the primary device.</p></li>
</ul>
<p>If there is only a small amount of fast storage available (for example, less
than a gigabyte), we recommend using the available space as a WAL device. But
if more fast storage is available, it makes more sense to provision a DB
device. Because the BlueStore journal is always placed on the fastest device
available, using a DB device provides the same benefit that using a WAL device
would, while <em>also</em> allowing additional metadata to be stored off the primary
device (provided that it fits). DB devices make this possible because whenever
a DB device is specified but an explicit WAL device is not, the WAL will be
implicitly colocated with the DB on the faster device.</p>
<p>To provision a single-device (colocated) BlueStore OSD, run the following
command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt1:before {
  content: "$ ";
}
</style><span class="prompt1">ceph-volume<span class="w"> </span>lvm<span class="w"> </span>prepare<span class="w"> </span>--bluestore<span class="w"> </span>--data<span class="w"> </span>&lt;device&gt;</span>
</pre></div></div><p>To specify a WAL device or DB device, run the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph-volume<span class="w"> </span>lvm<span class="w"> </span>prepare<span class="w"> </span>--bluestore<span class="w"> </span>--data<span class="w"> </span>&lt;device&gt;<span class="w"> </span>--block.wal<span class="w"> </span>&lt;wal-device&gt;<span class="w"> </span>--block.db<span class="w"> </span>&lt;db-device&gt;</span>
</pre></div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The option <code class="docutils literal notranslate"><span class="pre">--data</span></code> can take as its argument any of the the
following devices: logical volumes specified using <em>vg/lv</em> notation,
existing logical volumes, and GPT partitions.</p>
</div>
<section id="provisioning-strategies">
<h3>Provisioning strategies<a class="headerlink" href="#provisioning-strategies" title="Permalink to this heading">¶</a></h3>
<p>BlueStore differs from Filestore in that there are several ways to deploy a
BlueStore OSD. However, the overall deployment strategy for BlueStore can be
clarified by examining just these two common arrangements:</p>
<section id="block-data-only">
<span id="bluestore-single-type-device-config"></span><h4><strong>block (data) only</strong><a class="headerlink" href="#block-data-only" title="Permalink to this heading">¶</a></h4>
<p>If all devices are of the same type (for example, they are all HDDs), and if
there are no fast devices available for the storage of metadata, then it makes
sense to specify the block device only and to leave <code class="docutils literal notranslate"><span class="pre">block.db</span></code> and
<code class="docutils literal notranslate"><span class="pre">block.wal</span></code> unseparated. The <a class="reference internal" href="../../../ceph-volume/lvm/#ceph-volume-lvm"><span class="std std-ref">lvm</span></a> command for a single
<code class="docutils literal notranslate"><span class="pre">/dev/sda</span></code> device is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph-volume<span class="w"> </span>lvm<span class="w"> </span>create<span class="w"> </span>--bluestore<span class="w"> </span>--data<span class="w"> </span>/dev/sda</span>
</pre></div></div><p>If the devices to be used for a BlueStore OSD are pre-created logical volumes,
then the <a class="reference internal" href="../../../ceph-volume/lvm/#ceph-volume-lvm"><span class="std std-ref">lvm</span></a> call for an logical volume named
<code class="docutils literal notranslate"><span class="pre">ceph-vg/block-lv</span></code> is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph-volume<span class="w"> </span>lvm<span class="w"> </span>create<span class="w"> </span>--bluestore<span class="w"> </span>--data<span class="w"> </span>ceph-vg/block-lv</span>
</pre></div></div></section>
<section id="block-and-block-db">
<span id="bluestore-mixed-device-config"></span><h4><strong>block and block.db</strong><a class="headerlink" href="#block-and-block-db" title="Permalink to this heading">¶</a></h4>
<p>If you have a mix of fast and slow devices (for example, SSD or HDD), then we
recommend placing <code class="docutils literal notranslate"><span class="pre">block.db</span></code> on the faster device while <code class="docutils literal notranslate"><span class="pre">block</span></code> (that is,
the data) is stored on the slower device (that is, the rotational drive).</p>
<p>You must create these volume groups and these logical volumes manually. as The
<code class="docutils literal notranslate"><span class="pre">ceph-volume</span></code> tool is currently unable to do so [create them?] automatically.</p>
<p>The following procedure illustrates the manual creation of volume groups and
logical volumes.  For this example, we shall assume four rotational drives
(<code class="docutils literal notranslate"><span class="pre">sda</span></code>, <code class="docutils literal notranslate"><span class="pre">sdb</span></code>, <code class="docutils literal notranslate"><span class="pre">sdc</span></code>, and <code class="docutils literal notranslate"><span class="pre">sdd</span></code>) and one (fast) SSD (<code class="docutils literal notranslate"><span class="pre">sdx</span></code>). First,
to create the volume groups, run the following commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">vgcreate<span class="w"> </span>ceph-block-0<span class="w"> </span>/dev/sda</span>
<span class="prompt1">vgcreate<span class="w"> </span>ceph-block-1<span class="w"> </span>/dev/sdb</span>
<span class="prompt1">vgcreate<span class="w"> </span>ceph-block-2<span class="w"> </span>/dev/sdc</span>
<span class="prompt1">vgcreate<span class="w"> </span>ceph-block-3<span class="w"> </span>/dev/sdd</span>
</pre></div></div><p>Next, to create the logical volumes for <code class="docutils literal notranslate"><span class="pre">block</span></code>, run the following commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">lvcreate<span class="w"> </span>-l<span class="w"> </span><span class="m">100</span>%FREE<span class="w"> </span>-n<span class="w"> </span>block-0<span class="w"> </span>ceph-block-0</span>
<span class="prompt1">lvcreate<span class="w"> </span>-l<span class="w"> </span><span class="m">100</span>%FREE<span class="w"> </span>-n<span class="w"> </span>block-1<span class="w"> </span>ceph-block-1</span>
<span class="prompt1">lvcreate<span class="w"> </span>-l<span class="w"> </span><span class="m">100</span>%FREE<span class="w"> </span>-n<span class="w"> </span>block-2<span class="w"> </span>ceph-block-2</span>
<span class="prompt1">lvcreate<span class="w"> </span>-l<span class="w"> </span><span class="m">100</span>%FREE<span class="w"> </span>-n<span class="w"> </span>block-3<span class="w"> </span>ceph-block-3</span>
</pre></div></div><p>Because there are four HDDs, there will be four OSDs. Supposing that there is a
200GB SSD in <code class="docutils literal notranslate"><span class="pre">/dev/sdx</span></code>, we can create four 50GB logical volumes by running
the following commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">vgcreate<span class="w"> </span>ceph-db-0<span class="w"> </span>/dev/sdx</span>
<span class="prompt1">lvcreate<span class="w"> </span>-L<span class="w"> </span>50GB<span class="w"> </span>-n<span class="w"> </span>db-0<span class="w"> </span>ceph-db-0</span>
<span class="prompt1">lvcreate<span class="w"> </span>-L<span class="w"> </span>50GB<span class="w"> </span>-n<span class="w"> </span>db-1<span class="w"> </span>ceph-db-0</span>
<span class="prompt1">lvcreate<span class="w"> </span>-L<span class="w"> </span>50GB<span class="w"> </span>-n<span class="w"> </span>db-2<span class="w"> </span>ceph-db-0</span>
<span class="prompt1">lvcreate<span class="w"> </span>-L<span class="w"> </span>50GB<span class="w"> </span>-n<span class="w"> </span>db-3<span class="w"> </span>ceph-db-0</span>
</pre></div></div><p>Finally, to create the four OSDs, run the following commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph-volume<span class="w"> </span>lvm<span class="w"> </span>create<span class="w"> </span>--bluestore<span class="w"> </span>--data<span class="w"> </span>ceph-block-0/block-0<span class="w"> </span>--block.db<span class="w"> </span>ceph-db-0/db-0</span>
<span class="prompt1">ceph-volume<span class="w"> </span>lvm<span class="w"> </span>create<span class="w"> </span>--bluestore<span class="w"> </span>--data<span class="w"> </span>ceph-block-1/block-1<span class="w"> </span>--block.db<span class="w"> </span>ceph-db-0/db-1</span>
<span class="prompt1">ceph-volume<span class="w"> </span>lvm<span class="w"> </span>create<span class="w"> </span>--bluestore<span class="w"> </span>--data<span class="w"> </span>ceph-block-2/block-2<span class="w"> </span>--block.db<span class="w"> </span>ceph-db-0/db-2</span>
<span class="prompt1">ceph-volume<span class="w"> </span>lvm<span class="w"> </span>create<span class="w"> </span>--bluestore<span class="w"> </span>--data<span class="w"> </span>ceph-block-3/block-3<span class="w"> </span>--block.db<span class="w"> </span>ceph-db-0/db-3</span>
</pre></div></div><p>After this procedure is finished, there should be four OSDs, <code class="docutils literal notranslate"><span class="pre">block</span></code> should
be on the four HDDs, and each HDD should have a 50GB logical volume
(specifically, a DB device) on the shared SSD.</p>
</section>
</section>
</section>
<section id="sizing">
<h2>Sizing<a class="headerlink" href="#sizing" title="Permalink to this heading">¶</a></h2>
<p>When using a <a class="reference internal" href="#bluestore-mixed-device-config"><span class="std std-ref">mixed spinning-and-solid-drive setup</span></a>, it is important to make a large enough
<code class="docutils literal notranslate"><span class="pre">block.db</span></code> logical volume for BlueStore. The logical volumes associated with
<code class="docutils literal notranslate"><span class="pre">block.db</span></code> should have logical volumes that are <em>as large as possible</em>.</p>
<p>It is generally recommended that the size of <code class="docutils literal notranslate"><span class="pre">block.db</span></code> be somewhere between
1% and 4% of the size of <code class="docutils literal notranslate"><span class="pre">block</span></code>. For RGW workloads, it is recommended that
the <code class="docutils literal notranslate"><span class="pre">block.db</span></code> be at least 4% of the <code class="docutils literal notranslate"><span class="pre">block</span></code> size, because RGW makes heavy
use of <code class="docutils literal notranslate"><span class="pre">block.db</span></code> to store metadata (in particular, omap keys). For example,
if the <code class="docutils literal notranslate"><span class="pre">block</span></code> size is 1TB, then <code class="docutils literal notranslate"><span class="pre">block.db</span></code> should have a size of at least
40GB. For RBD workloads, however, <code class="docutils literal notranslate"><span class="pre">block.db</span></code> usually needs no more than 1% to
2% of the <code class="docutils literal notranslate"><span class="pre">block</span></code> size.</p>
<p>In older releases, internal level sizes are such that the DB can fully utilize
only those specific partition / logical volume sizes that correspond to sums of
L0, L0+L1, L1+L2, and so on–that is, given default settings, sizes of roughly
3GB, 30GB, 300GB, and so on. Most deployments do not substantially benefit from
sizing that accommodates L3 and higher, though DB compaction can be facilitated
by doubling these figures to 6GB, 60GB, and 600GB.</p>
<p>Improvements in Nautilus 14.2.12, Octopus 15.2.6, and subsequent releases allow
for better utilization of arbitrarily-sized DB devices. Moreover, the Pacific
release brings experimental dynamic-level support. Because of these advances,
users of older releases might want to plan ahead by provisioning larger DB
devices today so that the benefits of scale can be realized when upgrades are
made in the future.</p>
<p>When <em>not</em> using a mix of fast and slow devices, there is no requirement to
create separate logical volumes for <code class="docutils literal notranslate"><span class="pre">block.db</span></code> or <code class="docutils literal notranslate"><span class="pre">block.wal</span></code>. BlueStore
will automatically colocate these devices within the space of <code class="docutils literal notranslate"><span class="pre">block</span></code>.</p>
</section>
<section id="automatic-cache-sizing">
<h2>Automatic Cache Sizing<a class="headerlink" href="#automatic-cache-sizing" title="Permalink to this heading">¶</a></h2>
<p>BlueStore can be configured to automatically resize its caches, provided that
certain conditions are met: TCMalloc must be configured as the memory allocator
and the <code class="docutils literal notranslate"><span class="pre">bluestore_cache_autotune</span></code> configuration option must be enabled (note
that it is currently enabled by default). When automatic cache sizing is in
effect, BlueStore attempts to keep OSD heap-memory usage under a certain target
size (as determined by <code class="docutils literal notranslate"><span class="pre">osd_memory_target</span></code>). This approach makes use of a
best-effort algorithm and caches do not shrink smaller than the size defined by
the value of <code class="docutils literal notranslate"><span class="pre">osd_memory_cache_min</span></code>. Cache ratios are selected in accordance
with a hierarchy of priorities.  But if priority information is not available,
the values specified in the <code class="docutils literal notranslate"><span class="pre">bluestore_cache_meta_ratio</span></code> and
<code class="docutils literal notranslate"><span class="pre">bluestore_cache_kv_ratio</span></code> options are used as fallback cache ratios.</p>
</section>
<section id="manual-cache-sizing">
<h2>Manual Cache Sizing<a class="headerlink" href="#manual-cache-sizing" title="Permalink to this heading">¶</a></h2>
<p>The amount of memory consumed by each OSD to be used for its BlueStore cache is
determined by the <code class="docutils literal notranslate"><span class="pre">bluestore_cache_size</span></code> configuration option. If that option
has not been specified (that is, if it remains at 0), then Ceph uses a
different configuration option to determine the default memory budget:
<code class="docutils literal notranslate"><span class="pre">bluestore_cache_size_hdd</span></code> if the primary device is an HDD, or
<code class="docutils literal notranslate"><span class="pre">bluestore_cache_size_ssd</span></code> if the primary device is an SSD.</p>
<p>BlueStore and the rest of the Ceph OSD daemon make every effort to work within
this memory budget. Note that in addition to the configured cache size, there
is also memory consumed by the OSD itself. There is additional utilization due
to memory fragmentation and other allocator overhead.</p>
<p>The configured cache-memory budget can be used to store the following types of
things:</p>
<ul class="simple">
<li><p>Key/Value metadata (that is, RocksDB’s internal cache)</p></li>
<li><p>BlueStore metadata</p></li>
<li><p>BlueStore data (that is, recently read or recently written object data)</p></li>
</ul>
<p>Cache memory usage is governed by the configuration options
<code class="docutils literal notranslate"><span class="pre">bluestore_cache_meta_ratio</span></code> and <code class="docutils literal notranslate"><span class="pre">bluestore_cache_kv_ratio</span></code>.  The fraction
of the cache that is reserved for data is governed by both the effective
BlueStore cache size (which depends on the relevant
<code class="docutils literal notranslate"><span class="pre">bluestore_cache_size[_ssd|_hdd]</span></code> option and the device class of the primary
device) and the “meta” and “kv” ratios.  This data fraction can be calculated
with the following formula: <code class="docutils literal notranslate"><span class="pre">&lt;effective_cache_size&gt;</span> <span class="pre">*</span> <span class="pre">(1</span> <span class="pre">-</span>
<span class="pre">bluestore_cache_meta_ratio</span> <span class="pre">-</span> <span class="pre">bluestore_cache_kv_ratio)</span></code>.</p>
</section>
<section id="checksums">
<h2>Checksums<a class="headerlink" href="#checksums" title="Permalink to this heading">¶</a></h2>
<p>BlueStore checksums all metadata and all data written to disk. Metadata
checksumming is handled by RocksDB and uses the <cite>crc32c</cite> algorithm. By
contrast, data checksumming is handled by BlueStore and can use either
<cite>crc32c</cite>, <cite>xxhash32</cite>, or <cite>xxhash64</cite>. Nonetheless, <cite>crc32c</cite> is the default
checksum algorithm and it is suitable for most purposes.</p>
<p>Full data checksumming increases the amount of metadata that BlueStore must
store and manage. Whenever possible (for example, when clients hint that data
is written and read sequentially), BlueStore will checksum larger blocks. In
many cases, however, it must store a checksum value (usually 4 bytes) for every
4 KB block of data.</p>
<p>It is possible to obtain a smaller checksum value by truncating the checksum to
one or two bytes and reducing the metadata overhead.  A drawback of this
approach is that it increases the probability of a random error going
undetected: about one in four billion given a 32-bit (4 byte) checksum, 1 in
65,536 given a 16-bit (2 byte) checksum, and 1 in 256 given an 8-bit (1 byte)
checksum. To use the smaller checksum values, select <cite>crc32c_16</cite> or <cite>crc32c_8</cite>
as the checksum algorithm.</p>
<p>The <em>checksum algorithm</em> can be specified either via a per-pool <code class="docutils literal notranslate"><span class="pre">csum_type</span></code>
configuration option or via the global configuration option. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>&lt;pool-name&gt;<span class="w"> </span>csum_type<span class="w"> </span>&lt;algorithm&gt;</span>
</pre></div></div></section>
<section id="inline-compression">
<h2>Inline Compression<a class="headerlink" href="#inline-compression" title="Permalink to this heading">¶</a></h2>
<p>BlueStore supports inline compression using <cite>snappy</cite>, <cite>zlib</cite>, <cite>lz4</cite>, or <cite>zstd</cite>.</p>
<p>Whether data in BlueStore is compressed is determined by two factors: (1) the
<em>compression mode</em> and (2) any client hints associated with a write operation.
The compression modes are as follows:</p>
<ul class="simple">
<li><p><strong>none</strong>: Never compress data.</p></li>
<li><p><strong>passive</strong>: Do not compress data unless the write operation has a
<em>compressible</em> hint set.</p></li>
<li><p><strong>aggressive</strong>: Do compress data unless the write operation has an
<em>incompressible</em> hint set.</p></li>
<li><p><strong>force</strong>: Try to compress data no matter what.</p></li>
</ul>
<p>For more information about the <em>compressible</em> and <em>incompressible</em> I/O hints,
see <a class="reference internal" href="../../api/librados/#c.rados_set_alloc_hint" title="rados_set_alloc_hint"><code class="xref c c-func docutils literal notranslate"><span class="pre">rados_set_alloc_hint()</span></code></a>.</p>
<p>Note that data in Bluestore will be compressed only if the data chunk will be
sufficiently reduced in size (as determined by the <code class="docutils literal notranslate"><span class="pre">bluestore</span> <span class="pre">compression</span>
<span class="pre">required</span> <span class="pre">ratio</span></code> setting). No matter which compression modes have been used, if
the data chunk is too big, then it will be discarded and the original
(uncompressed) data will be stored instead. For example, if <code class="docutils literal notranslate"><span class="pre">bluestore</span>
<span class="pre">compression</span> <span class="pre">required</span> <span class="pre">ratio</span></code> is set to <code class="docutils literal notranslate"><span class="pre">.7</span></code>, then data compression will take
place only if the size of the compressed data is no more than 70% of the size
of the original data.</p>
<p>The <em>compression mode</em>, <em>compression algorithm</em>, <em>compression required ratio</em>,
<em>min blob size</em>, and <em>max blob size</em> settings can be specified either via a
per-pool property or via a global config option. To specify pool properties,
run the following commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>&lt;pool-name&gt;<span class="w"> </span>compression_algorithm<span class="w"> </span>&lt;algorithm&gt;</span>
<span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>&lt;pool-name&gt;<span class="w"> </span>compression_mode<span class="w"> </span>&lt;mode&gt;</span>
<span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>&lt;pool-name&gt;<span class="w"> </span>compression_required_ratio<span class="w"> </span>&lt;ratio&gt;</span>
<span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>&lt;pool-name&gt;<span class="w"> </span>compression_min_blob_size<span class="w"> </span>&lt;size&gt;</span>
<span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>&lt;pool-name&gt;<span class="w"> </span>compression_max_blob_size<span class="w"> </span>&lt;size&gt;</span>
</pre></div></div></section>
<section id="rocksdb-sharding">
<span id="bluestore-rocksdb-sharding"></span><h2>RocksDB Sharding<a class="headerlink" href="#rocksdb-sharding" title="Permalink to this heading">¶</a></h2>
<p>BlueStore maintains several types of internal key-value data, all of which are
stored in RocksDB. Each data type in BlueStore is assigned a unique prefix.
Prior to the Pacific release, all key-value data was stored in a single RocksDB
column family: ‘default’. In Pacific and later releases, however, BlueStore can
divide key-value data into several RocksDB column families. BlueStore achieves
better caching and more precise compaction when keys are similar: specifically,
when keys have similar access frequency, similar modification frequency, and a
similar lifetime.  Under such conditions, performance is improved and less disk
space is required during compaction (because each column family is smaller and
is able to compact independently of the others).</p>
<p>OSDs deployed in Pacific or later releases use RocksDB sharding by default.
However, if Ceph has been upgraded to Pacific or a later version from a
previous version, sharding is disabled on any OSDs that were created before
Pacific.</p>
<p>To enable sharding and apply the Pacific defaults to a specific OSD, stop the
OSD and run the following command:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt2:before {
  content: "# ";
}
</style><span class="prompt2">ceph-bluestore-tool<span class="w"> </span><span class="se">\</span>
<span class="w"> </span>--path<span class="w"> </span>&lt;data<span class="w"> </span>path&gt;<span class="w"> </span><span class="se">\</span>
<span class="w"> </span>--sharding<span class="o">=</span><span class="s2">&quot;m(3) p(3,0-12) O(3,0-13)=block_cache={type=binned_lru} L P&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w"> </span>reshard</span>
</pre></div></div></div></blockquote>
</section>
<section id="spdk-usage">
<h2>SPDK Usage<a class="headerlink" href="#spdk-usage" title="Permalink to this heading">¶</a></h2>
<p>To use the SPDK driver for NVMe devices, you must first prepare your system.
See <a class="reference external" href="http://www.spdk.io/doc/getting_started.html#getting_started_examples">SPDK document</a>.</p>
<p>SPDK offers a script that will configure the device automatically. Run this
script with root permissions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sudo<span class="w"> </span>src/spdk/scripts/setup.sh</span>
</pre></div></div><p>You will need to specify the subject NVMe device’s device selector with the
“spdk:” prefix for <code class="docutils literal notranslate"><span class="pre">bluestore_block_path</span></code>.</p>
<p>In the following example, you first find the device selector of an Intel NVMe
SSD by running the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">lspci<span class="w"> </span>-mm<span class="w"> </span>-n<span class="w"> </span>-d<span class="w"> </span>-d<span class="w"> </span><span class="m">8086</span>:0953</span>
</pre></div></div><p>The form of the device selector is either <code class="docutils literal notranslate"><span class="pre">DDDD:BB:DD.FF</span></code> or
<code class="docutils literal notranslate"><span class="pre">DDDD.BB.DD.FF</span></code>.</p>
<p>Next, supposing that <code class="docutils literal notranslate"><span class="pre">0000:01:00.0</span></code> is the device selector found in the
output of the <code class="docutils literal notranslate"><span class="pre">lspci</span></code> command, you can specify the device selector by running
the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bluestore_block_path</span> <span class="o">=</span> <span class="s2">&quot;spdk:trtype:pcie traddr:0000:01:00.0&quot;</span>
</pre></div>
</div>
<p>You may also specify a remote NVMeoF target over the TCP transport, as in the
following example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bluestore_block_path</span> <span class="o">=</span> <span class="s2">&quot;spdk:trtype:tcp traddr:10.67.110.197 trsvcid:4420 subnqn:nqn.2019-02.io.spdk:cnode1&quot;</span>
</pre></div>
</div>
<p>To run multiple SPDK instances per node, you must make sure each instance uses
its own DPDK memory by specifying for each instance the amount of DPDK memory
(in MB) that the instance will use.</p>
<p>In most cases, a single device can be used for data, DB, and WAL. We describe
this strategy as <em>colocating</em> these components. Be sure to enter the below
settings to ensure that all I/Os are issued through SPDK:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bluestore_block_db_path</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<span class="n">bluestore_block_db_size</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">bluestore_block_wal_path</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<span class="n">bluestore_block_wal_size</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
<p>If these settings are not entered, then the current implementation will
populate the SPDK map files with kernel file system symbols and will use the
kernel driver to issue DB/WAL I/Os.</p>
</section>
<section id="minimum-allocation-size">
<h2>Minimum Allocation Size<a class="headerlink" href="#minimum-allocation-size" title="Permalink to this heading">¶</a></h2>
<p>There is a configured minimum amount of storage that BlueStore allocates on an
underlying storage device. In practice, this is the least amount of capacity
that even a tiny RADOS object can consume on each OSD’s primary device. The
configuration option in question– <code class="docutils literal notranslate"><span class="pre">bluestore_min_alloc_size</span></code> –derives
its value from the value of either <code class="docutils literal notranslate"><span class="pre">bluestore_min_alloc_size_hdd</span></code> or
<code class="docutils literal notranslate"><span class="pre">bluestore_min_alloc_size_ssd</span></code>, depending on the OSD’s <code class="docutils literal notranslate"><span class="pre">rotational</span></code>
attribute. Thus if an OSD is created on an HDD, BlueStore is initialized with
the current value of <code class="docutils literal notranslate"><span class="pre">bluestore_min_alloc_size_hdd</span></code>; but with SSD OSDs
(including NVMe devices), Bluestore is initialized with the current value of
<code class="docutils literal notranslate"><span class="pre">bluestore_min_alloc_size_ssd</span></code>.</p>
<p>In Mimic and earlier releases, the default values were 64KB for rotational
media (HDD) and 16KB for non-rotational media (SSD). The Octopus release
changed the the default value for non-rotational media (SSD) to 4KB, and the
Pacific release changed the default value for rotational media (HDD) to 4KB.</p>
<p>These changes were driven by space amplification that was experienced by Ceph
RADOS GateWay (RGW) deployments that hosted large numbers of small files
(S3/Swift objects).</p>
<p>For example, when an RGW client stores a 1 KB S3 object, that object is written
to a single RADOS object. In accordance with the default
<code class="docutils literal notranslate"><span class="pre">min_alloc_size</span></code> value, 4 KB of underlying drive space is allocated.
This means that roughly 3 KB (that is, 4 KB minus 1 KB) is allocated but never
used: this corresponds to 300% overhead or 25% efficiency. Similarly, a 5 KB
user object will be stored as two RADOS objects, a 4 KB RADOS object and a 1 KB
RADOS object, with the result that 4KB of device capacity is stranded. In this
case, however, the overhead percentage is much smaller. Think of this in terms
of the remainder from a modulus operation. The overhead <em>percentage</em> thus
decreases rapidly as object size increases.</p>
<p>There is an additional subtlety that is easily missed: the amplification
phenomenon just described takes place for <em>each</em> replica. For example, when
using the default of three copies of data (3R), a 1 KB S3 object actually
strands roughly 9 KB of storage device capacity. If erasure coding (EC) is used
instead of replication, the amplification might be even higher: for a <code class="docutils literal notranslate"><span class="pre">k=4,</span>
<span class="pre">m=2</span></code> pool, our 1 KB S3 object allocates 24 KB (that is, 4 KB multiplied by 6)
of device capacity.</p>
<p>When an RGW bucket pool contains many relatively large user objects, the effect
of this phenomenon is often negligible. However, with deployments that can
expect a significant fraction of relatively small user objects, the effect
should be taken into consideration.</p>
<p>The 4KB default value aligns well with conventional HDD and SSD devices.
However, certain novel coarse-IU (Indirection Unit) QLC SSDs perform and wear
best when <code class="docutils literal notranslate"><span class="pre">bluestore_min_alloc_size_ssd</span></code> is specified at OSD creation
to match the device’s IU: this might be 8KB, 16KB, or even 64KB.  These novel
storage drives can achieve read performance that is competitive with that of
conventional TLC SSDs and write performance that is faster than that of HDDs,
with higher density and lower cost than TLC SSDs.</p>
<p>Note that when creating OSDs on these novel devices, one must be careful to
apply the non-default value only to appropriate devices, and not to
conventional HDD and SSD devices. Error can be avoided through careful ordering
of OSD creation, with custom OSD device classes, and especially by the use of
central configuration <em>masks</em>.</p>
<p>In Quincy and later releases, you can use the
<code class="docutils literal notranslate"><span class="pre">bluestore_use_optimal_io_size_for_min_alloc_size</span></code> option to allow
automatic discovery of the correct value as each OSD is created. Note that the
use of <code class="docutils literal notranslate"><span class="pre">bcache</span></code>, <code class="docutils literal notranslate"><span class="pre">OpenCAS</span></code>, <code class="docutils literal notranslate"><span class="pre">dmcrypt</span></code>, <code class="docutils literal notranslate"><span class="pre">ATA</span> <span class="pre">over</span> <span class="pre">Ethernet</span></code>, <cite>iSCSI</cite>, or
other device-layering and abstraction technologies might confound the
determination of correct values. Moreover, OSDs deployed on top of VMware
storage have sometimes been found to report a <code class="docutils literal notranslate"><span class="pre">rotational</span></code> attribute that
does not match the underlying hardware.</p>
<p>We suggest inspecting such OSDs at startup via logs and admin sockets in order
to ensure that their behavior is correct. Be aware that this kind of inspection
might not work as expected with older kernels.  To check for this issue,
examine the presence and value of <code class="docutils literal notranslate"><span class="pre">/sys/block/&lt;drive&gt;/queue/optimal_io_size</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When running Reef or a later Ceph release, the <code class="docutils literal notranslate"><span class="pre">min_alloc_size</span></code>
baked into each OSD is conveniently reported by <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">metadata</span></code>.</p>
</div>
<p>To inspect a specific OSD, run the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt2">ceph<span class="w"> </span>osd<span class="w"> </span>metadata<span class="w"> </span>osd.1701<span class="w"> </span><span class="p">|</span><span class="w"> </span>egrep<span class="w"> </span>rotational<span class="se">\|</span>alloc</span>
</pre></div></div><p>This space amplification might manifest as an unusually high ratio of raw to
stored data as reported by <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">df</span></code>. There might also be <code class="docutils literal notranslate"><span class="pre">%USE</span></code> / <code class="docutils literal notranslate"><span class="pre">VAR</span></code>
values reported by <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">df</span></code> that are unusually high in comparison to
other, ostensibly identical, OSDs. Finally, there might be unexpected balancer
behavior in pools that use OSDs that have mismatched <code class="docutils literal notranslate"><span class="pre">min_alloc_size</span></code> values.</p>
<p>This BlueStore attribute takes effect <em>only</em> at OSD creation; if the attribute
is changed later, a specific OSD’s behavior will not change unless and until
the OSD is destroyed and redeployed with the appropriate option value(s).
Upgrading to a later Ceph release will <em>not</em> change the value used by OSDs that
were deployed under older releases or with other settings.</p>
</section>
<section id="dsa-data-streaming-accelerator-usage">
<h2>DSA (Data Streaming Accelerator) Usage<a class="headerlink" href="#dsa-data-streaming-accelerator-usage" title="Permalink to this heading">¶</a></h2>
<p>If you want to use the DML library to drive the DSA device for offloading
read/write operations on persistent memory (PMEM) in BlueStore, you need to
install <a class="reference external" href="https://github.com/intel/dml">DML</a> and the <a class="reference external" href="https://github.com/intel/idxd-config">idxd-config</a> library. This will work only on machines
that have a SPR (Sapphire Rapids) CPU.</p>
<p>After installing the DML software, configure the shared work queues (WQs) with
reference to the following WQ configuration example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">accel-config<span class="w"> </span>config-wq<span class="w"> </span>--group-id<span class="o">=</span><span class="m">1</span><span class="w"> </span>--mode<span class="o">=</span>shared<span class="w"> </span>--wq-size<span class="o">=</span><span class="m">16</span><span class="w"> </span>--threshold<span class="o">=</span><span class="m">15</span><span class="w"> </span>--type<span class="o">=</span>user<span class="w"> </span>--name<span class="o">=</span><span class="s2">&quot;myapp1&quot;</span><span class="w"> </span>--priority<span class="o">=</span><span class="m">10</span><span class="w"> </span>--block-on-fault<span class="o">=</span><span class="m">1</span><span class="w"> </span>dsa0/wq0.1</span>
<span class="prompt1">accel-config<span class="w"> </span>config-engine<span class="w"> </span>dsa0/engine0.1<span class="w"> </span>--group-id<span class="o">=</span><span class="m">1</span></span>
<span class="prompt1">accel-config<span class="w"> </span>enable-device<span class="w"> </span>dsa0</span>
<span class="prompt1">accel-config<span class="w"> </span>enable-wq<span class="w"> </span>dsa0/wq0.1</span>
</pre></div></div></section>
</section>



            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../">
              <img class="logo" src="../../../_static/logo.png" alt="Logo"/>
            </a></p>
<h3><a href="../../../">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../start/intro/">Intro to Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install/">Installing Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cephadm/">Cephadm</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../">Ceph Storage Cluster</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../">Configuration</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../storage-devices/">Storage devices</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ceph-conf/">Configuring Ceph</a></li>
<li class="toctree-l3"><a class="reference internal" href="../common/">Common Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../common/#networks">Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../common/#monitors">Monitors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../common/#authentication">Authentication</a></li>
<li class="toctree-l3"><a class="reference internal" href="../common/#osds">OSDs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../common/#heartbeats">Heartbeats</a></li>
<li class="toctree-l3"><a class="reference internal" href="../common/#logs-debugging">Logs / Debugging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../common/#example-ceph-conf">Example ceph.conf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../common/#running-multiple-clusters-deprecated">Running Multiple Clusters (DEPRECATED)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../network-config-ref/">Network Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../msgr2/">Messenger v2 protocol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auth-config-ref/">Auth Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mon-config-ref/">Monitor Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mon-lookup-dns/">Looking up Monitors through DNS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mon-osd-interaction/">Heartbeat Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../osd-config-ref/">OSD Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mclock-config-ref/">DmClock Settings</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">BlueStore Settings</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#devices">Devices</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#provisioning-strategies">Provisioning strategies</a><ul>
<li class="toctree-l6"><a class="reference internal" href="#block-data-only"><strong>block (data) only</strong></a></li>
<li class="toctree-l6"><a class="reference internal" href="#block-and-block-db"><strong>block and block.db</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#sizing">Sizing</a></li>
<li class="toctree-l4"><a class="reference internal" href="#automatic-cache-sizing">Automatic Cache Sizing</a></li>
<li class="toctree-l4"><a class="reference internal" href="#manual-cache-sizing">Manual Cache Sizing</a></li>
<li class="toctree-l4"><a class="reference internal" href="#checksums">Checksums</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inline-compression">Inline Compression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#rocksdb-sharding">RocksDB Sharding</a></li>
<li class="toctree-l4"><a class="reference internal" href="#spdk-usage">SPDK Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#minimum-allocation-size">Minimum Allocation Size</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dsa-data-streaming-accelerator-usage">DSA (Data Streaming Accelerator) Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../filestore-config-ref/">FileStore Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../journal-ref/">Journal Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pool-pg-config-ref/">Pool, PG &amp; CRUSH Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ms-ref/">Messaging Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../general-config-ref/">General Settings</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../cephadm/">Deployment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/">Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../man/">Man Pages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../troubleshooting/">Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/">APIs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../cephfs/">Ceph File System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rbd/">Ceph Block Device</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../radosgw/">Ceph Object Gateway</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mgr/">Ceph Manager Daemon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mgr/dashboard/">Ceph Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architecture/">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dev/developer_guide/">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dev/internals/">Ceph Internals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../governance/">Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../foundation/">Ceph Foundation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.ceph.com/en/latest/releases/general/">Ceph Releases (general)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.ceph.com/en/latest/releases/">Ceph Releases (index)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../security/">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../glossary/">Glossary</a></li>
</ul>


<!-- ugly kludge to make genindex look like it's part of the toc -->
<ul style="margin-top: -10px"><li class="toctree-l1"><a class="reference internal" href="../../../genindex/">Index</a></li></ul>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search/" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../http-routingtable/" title="HTTP Routing Table"
             >routing table</a> |</li>
        <li class="right" >
          <a href="../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../filestore-config-ref/" title="Filestore Config Reference"
             >next</a> |</li>
        <li class="right" >
          <a href="../mclock-config-ref/" title="mClock Config Reference"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../">Ceph Documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../" >Ceph Storage Cluster</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../" >Configuration</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">BlueStore Configuration Reference</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).
    </div>
  </body>
</html>